{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5720b983f958cbdc",
   "metadata": {},
   "source": [
    "---\n",
    "date: \"2025-1-7T14:57:00.00Z\"\n",
    "description: \"Trying out PydanticAI\"\n",
    "published: true\n",
    "tags:\n",
    "  - python\n",
    "  - llm\n",
    "  - anthropic\n",
    "  - bedrock\n",
    "time_to_read: 5\n",
    "title: \"PydanticAI + Amazon Bedrock\"\n",
    "type: post\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad5b106-e3d1-4c69-a0af-74f2c8266693",
   "metadata": {},
   "source": [
    "Recently, there has been alot of noise about LLM agents. I've observed customers are frequently stuck in the undifferentiated stages of starting AI projects when choosing between competing tools and frameworks. This is backed by a recent blog from Anthropic [Building Effective Agents](https://www.anthropic.com/research/building-effective-agents) recommending to work at lower levels of abstraction.\n",
    "\n",
    "Recently I've been enjoying building with the brand new PydanticAI library which has just yesterday released support for Amazon Bedrock in release `0.0.36`. My first impressions are that this is at just the right level of abstraction for building LLM powered systems. In this post, I'll implement each of the Anthropic agent patterns with PydanticAI. Note this is very early days for this framework so be wary of breaking changes.\n",
    "\n",
    "Please remember the caveat that a large class of problems can be solved with simpler algorithms. You should consider those first since they may well be more reliable, cheaper and more interpretable.\n",
    "\n",
    "0) [Hello World](#hello-world)\n",
    "1) [Pydantic Model Structured Output](#pydantic-model-structured-output)\n",
    "2) [Building block: The Augmented LLM](#the-augmented-llm-with-structured-output)\n",
    "4) [Workflow: Prompt chaining](#workflow-prompt-chaining)\n",
    "5) [Workflow: Routing](#workflow-routing)\n",
    "6) [Workflow: Parallelization](#workflow-parallelization)\n",
    "7) [Workflow: Evaluator-optimizer](#workflow-evaluator-optimizer)\n",
    "8) [Workflow: Orchestrator-workers](#workflow-orchestrator-workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07acc2c7",
   "metadata": {},
   "source": [
    "# Hello World\n",
    "Let's get started by installing `pydantic-ai` with bedrock support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "835a29b65df77b8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T11:12:05.693984Z",
     "start_time": "2025-02-24T11:12:05.054817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m187 packages\u001b[0m \u001b[2min 136ms\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K\u001b[2mAudited \u001b[1m184 packages\u001b[0m \u001b[2min 0.57ms\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv add 'pydantic-ai-slim[bedrock]' pydantic-graph boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4722949b",
   "metadata": {},
   "source": [
    "Now, assuming we have AWS credentials configured, we can make an LLM call to Amazon Bedrock. We'll use the `bedrock:us.amazon.nova-lite-v1:0` model for this blog. Amazon Nova Lite is a very low cost mid-sized multimodal model that is lightning fast for processing image, video, and text inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a260eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:57:32.519 agent run\n",
      "17:57:32.520   preparing model request params\n",
      "17:57:32.521   chat us.amazon.nova-lite-v1:0\n",
      "\"Hello, world!\" originates from the first example program in the 1978 book \"The C Programming Language\" by Brian Kernighan and Dennis Ritchie.\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    model='bedrock:us.amazon.nova-lite-v1:0',\n",
    "    system_prompt='Be concise, reply with one sentence.'\n",
    ")\n",
    "\n",
    "result = agent.run_sync('Where does \"hello world\" come from?')  \n",
    "print(result.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b8323d",
   "metadata": {},
   "source": [
    "## A note on Observability\n",
    "\n",
    "While we could scroll through the verbose logs to see every detail of the LLM interaction, it get's pretty unwieldy for longer interactions, especially when there are loops in the workflows. There will also be traces of interactions when we span multiple code paths from a single interaction. Furthermore, there will be application metrics we'll want to track over time. This is there observability comes in. There is a lot to learn from traditional software engineering here. If you want to see what good looks like then check out [Pydantic logfire](https://logfire.pydantic.dev/docs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801543ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m187 packages\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m184 packages\u001b[0m \u001b[2min 0.31ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "You are already logged in. \u001b[1m(\u001b[0mYour credentials are stored in \n",
      "\u001b[1;35m/Users/stephenhibbert/.logfire/\u001b[0m\u001b[1;95mdefault.toml\u001b[0m\u001b[1m)\u001b[0m\n",
      "Project configured successfully. You will be able to view it at: \n",
      "\u001b[4;94mhttps://logfire.pydantic.dev/stephenhibbert/stephenhib-blog\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mLogfire\u001b[0m project URL: \u001b]8;id=852728;https://logfire.pydantic.dev/stephenhibbert/stephenhib-blog\u001b\\\u001b[4;36mhttps://logfire.pydantic.dev/stephenhibbert/stephenhib-blog\u001b[0m\u001b]8;;\u001b\\\n"
     ]
    }
   ],
   "source": [
    "%uv add logfire\n",
    "\n",
    "!logfire auth\n",
    "!logfire projects use stephenhib-blog\n",
    "\n",
    "import logfire\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "logfire.configure(send_to_logfire='if-token-present')\n",
    "Agent.instrument_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1f34c4",
   "metadata": {},
   "source": [
    "![Logfire UI](../public/images/pydantic-ai/logfire.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e3a522-d071-4e92-ba62-f430f71ec9e2",
   "metadata": {},
   "source": [
    "## Pydantic Model Structured Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2802edbc02bf590a",
   "metadata": {},
   "source": [
    "Let's start with a simple structured output, one of the neat building blocks of PydanticAI. Read my [previous post](https://stephenhib.com/posts/llm-structured-output) if you're interested in the theory of how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da2e1f578c6f1398",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T11:12:09.772531Z",
     "start_time": "2025-02-24T11:12:07.039365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:58:04.724 agent run\n",
      "17:58:04.724   preparing model request params\n",
      "17:58:04.724   chat us.amazon.nova-lite-v1:0\n",
      "name='Luna' animal='Tiger' age=5 <class '__main__.Character'>\n",
      "5 <class 'int'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ModelRequest(parts=[SystemPromptPart(content=\"You're a fun, helpful assistant\", dynamic_ref=None, part_kind='system-prompt'), UserPromptPart(content='Create a fictional animal character', timestamp=datetime.datetime(2025, 3, 7, 17, 58, 4, 724734, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'),\n",
       " ModelResponse(parts=[TextPart(content=\"<thinking>I need to create a fictional animal character with a name, an animal type, and an age. I will use the 'final_result' tool to generate this character.</thinking>\\n\", part_kind='text'), ToolCallPart(tool_name='final_result', args={'name': 'Luna', 'animal': 'Tiger', 'age': 5}, tool_call_id='tooluse_nJ6fpLH-TaKdkUfTnSA8Yw', part_kind='tool-call')], model_name='us.amazon.nova-lite-v1:0', timestamp=datetime.datetime(2025, 3, 7, 17, 58, 5, 916352, tzinfo=datetime.timezone.utc), kind='response'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='final_result', content='Final result processed.', tool_call_id='tooluse_nJ6fpLH-TaKdkUfTnSA8Yw', timestamp=datetime.datetime(2025, 3, 7, 17, 58, 5, 917684, tzinfo=datetime.timezone.utc), part_kind='tool-return')], kind='request')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic_ai.models import ModelSettings\n",
    "from pydantic_ai import Agent\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # This allows for nested event loops in Jupyter Notebooks\n",
    "\n",
    "class Character(BaseModel):\n",
    "    name: str\n",
    "    animal: str\n",
    "    age: int\n",
    "\n",
    "agent = Agent(\n",
    "    model='bedrock:us.amazon.nova-lite-v1:0',\n",
    "    result_type=Character,\n",
    "    system_prompt=\"You're a fun, helpful assistant\"\n",
    ")\n",
    "\n",
    "result = agent.run_sync('Create a fictional animal character')\n",
    "my_animal = result.data\n",
    "print(my_animal, type(my_animal)) # This is a Pydantic data model\n",
    "print(my_animal.age, type(my_animal.age)) # And I can access model fields without parsing strings 😁\n",
    "result.new_messages() # print the conversation history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726cc6fc-162e-4780-b5d0-099e9a679e44",
   "metadata": {},
   "source": [
    "Note that PydanticAI abstracts every LLM call through it's [Agent](https://ai.pydantic.dev/agents/) class. This is different from Anthropic's definition of Agents but we won't be pedantic 🙄. I consider the Pydantic Agent class as a building block to create systems with agency but everyone seems to have their own definition. The takeaway for this first section is that we have a way to call an LLM in a convenient output format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16867fac1a4be611",
   "metadata": {},
   "source": [
    "## The Augmented LLM with Structured Output\n",
    "There are some situations where we want models to have access to external tools in order to generate a response. Tools **augment** the language model by providing external context. Common augmentations include:\n",
    "- Retrieval (RAG): query/results on an index of documents\n",
    "- Memory: read/write on a database of previous interactions\n",
    "- APIs: request/response on an API\n",
    "\n",
    "> ![The Augmented LLM](../public/images/pydantic-ai/aug-llm.webp)\n",
    "> Anthropic - Building effective agents\n",
    "\n",
    "However, since the tool is just a Python function, we can augment with anything we can implement in code! Let's see an example of an augmented LLM that has access to call code functions. In PydanticAI we can do this with the `@agent.tool_plain` decorator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b22b4c6a8deb5bfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T11:13:09.581679Z",
     "start_time": "2025-02-24T11:13:05.156774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:04:48.256 agent run\n",
      "18:04:48.256   preparing model request params\n",
      "18:04:48.257   chat us.amazon.nova-lite-v1:0\n",
      "18:04:49.330   running tools: roll_die\n",
      "18:04:49.331   preparing model request params\n",
      "18:04:49.332   chat us.amazon.nova-lite-v1:0\n",
      "<thinking>I have rolled the die and got a 2. The user's guess was 4, so I need to inform the user that they did not win.</thinking>\n",
      "\n",
      "\n",
      "\n",
      "Here is the result of the die roll: 2. Since your guess was 4, you are not a winner this time. Try again!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ModelRequest(parts=[SystemPromptPart(content=\"You're a dice game, you should roll the die and see if the number you get back matches the user's guess. If so, tell them they're a winner.\", dynamic_ref=None, part_kind='system-prompt'), UserPromptPart(content='My guess is 4', timestamp=datetime.datetime(2025, 3, 7, 18, 4, 48, 256934, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'),\n",
       " ModelResponse(parts=[TextPart(content=\"<thinking>The user has made a guess, and I need to roll a die to see if it matches the user's guess. I will use the tool 'roll_die' to simulate the roll of a six-sided die.</thinking>\\n\", part_kind='text'), ToolCallPart(tool_name='roll_die', args={}, tool_call_id='tooluse_6TiqYDSiQhizqDjW_jnN2A', part_kind='tool-call')], model_name='us.amazon.nova-lite-v1:0', timestamp=datetime.datetime(2025, 3, 7, 18, 4, 49, 329330, tzinfo=datetime.timezone.utc), kind='response'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='roll_die', content='2', tool_call_id='tooluse_6TiqYDSiQhizqDjW_jnN2A', timestamp=datetime.datetime(2025, 3, 7, 18, 4, 49, 331252, tzinfo=datetime.timezone.utc), part_kind='tool-return')], kind='request'),\n",
       " ModelResponse(parts=[TextPart(content=\"<thinking>I have rolled the die and got a 2. The user's guess was 4, so I need to inform the user that they did not win.</thinking>\\n\\n\\n\\nHere is the result of the die roll: 2. Since your guess was 4, you are not a winner this time. Try again!\", part_kind='text')], model_name='us.amazon.nova-lite-v1:0', timestamp=datetime.datetime(2025, 3, 7, 18, 4, 50, 263133, tzinfo=datetime.timezone.utc), kind='response')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # This allows for nested event loops in Jupyter Notebooks\n",
    "\n",
    "from pydantic_ai import Agent, RunContext\n",
    "\n",
    "agent = Agent(\n",
    "    model='bedrock:us.amazon.nova-lite-v1:0',\n",
    "    deps_type=str,\n",
    "    system_prompt=(\n",
    "        \"You're a dice game, you should roll the die and see if the number \"\n",
    "        \"you get back matches the user's guess. If so, tell them they're a winner.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "@agent.tool_plain\n",
    "def roll_die() -> str:\n",
    "    \"\"\"Roll a six-sided die and return the result.\"\"\"\n",
    "    return str(random.randint(1, 6))\n",
    "\n",
    "\n",
    "dice_result = agent.run_sync('My guess is 4')\n",
    "print(dice_result.data)\n",
    "dice_result.new_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b394b129-198a-46e1-8224-eb79e6aadfa9",
   "metadata": {},
   "source": [
    "You can see we made two LLM calls in the message logs. The first LLM request took the user query and predicted that it should use the `roll_die` tool. The second LLM generates the final response back to the user. Note if we ask a question about something off-topic that dosen't include a guess then we only have a single LLM call since the model does not decide to call the `roll_die()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35c077bf-a6b6-499d-a194-0ebbc09e8dd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T11:14:06.910967Z",
     "start_time": "2025-02-24T11:14:03.563921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:05:09.377 agent run\n",
      "18:05:09.377   preparing model request params\n",
      "18:05:09.378   chat us.amazon.nova-lite-v1:0\n",
      "<thinking> The User is asking about my current state. I do not have feelings or states but I can respond in a friendly manner to the User's question. </thinking>\n",
      "I'm just a program, so I don't have feelings, but I'm here and ready to help you with anything you need. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "result = agent.run_sync('How are you today?')\n",
    "print(result.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1dd79c-fc22-4fb6-8217-a3529e6d1996",
   "metadata": {},
   "source": [
    "Now we've implemented the Augmented LLM, we have a pretty general building block to apply to our workflows! In the above example we return a string, but it's really easy to specify other datastructures too by changing `result_type` to return our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a2a6a4d4c7e5acf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T11:15:19.757833Z",
     "start_time": "2025-02-24T11:15:17.444104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:05:27.560 agent run\n",
      "18:05:27.561   preparing model request params\n",
      "18:05:27.561   chat us.amazon.nova-lite-v1:0\n",
      "roll_guess=6 roll_result=3 message='Your guess was 6, but I rolled a 3. Better luck next time!'\n"
     ]
    }
   ],
   "source": [
    "class Result(BaseModel):\n",
    "    roll_guess: int\n",
    "    roll_result: int\n",
    "    message: str\n",
    "\n",
    "agent = Agent(\n",
    "    model='bedrock:us.amazon.nova-lite-v1:0',\n",
    "    result_type=Result,\n",
    "    model_settings=ModelSettings(\n",
    "        temperature=1 # set the temperature to 1 to stop rolling 4 all the time!\n",
    "    ),\n",
    "    system_prompt=(\n",
    "        \"You're a dice game, you should roll the die and see if the number \"\n",
    "        \"you get back matches the user's guess. If so, tell them they're a winner.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "dice_result = agent.run_sync('My guess is 6')\n",
    "print(dice_result.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b8ccb9-1bd1-4b7e-a8a6-d54971a1ae3e",
   "metadata": {},
   "source": [
    "# Workflow: Prompt chaining\n",
    "Let's apply our Augmented LLM building block to make a predefined workflow of chaining prompts together. Prompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. This can be applied in a situation when a task can be decomposed into fixed subtasks. It's also great for applying guardrails at known points in the workflow.\n",
    "\n",
    "> ![Prompt chaining](../public/images/pydantic-ai/chain.png)\n",
    "> Anthropic - Building effective agents\n",
    "\n",
    "Let's make a prompt chain with PydanticAI to build a joke generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "482c533e-ac02-4c3c-9e30-b439694ba1b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T11:30:23.917392Z",
     "start_time": "2025-02-24T11:30:12.482648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:15:11.352 joke_agent run\n",
      "18:15:11.354   preparing model request params\n",
      "18:15:11.354   chat anthropic.claude-3-sonnet-20240229-v1:0\n",
      "18:15:13.179 guardrail_agent run\n",
      "18:15:13.180   preparing model request params\n",
      "18:15:13.180   chat anthropic.claude-3-sonnet-20240229-v1:0\n",
      "18:15:14.310 joke_explainer_agent run\n",
      "18:15:14.310   preparing model request params\n",
      "18:15:14.311   chat anthropic.claude-3-sonnet-20240229-v1:0\n",
      "The joke \"What kind of cheese can never be yours?\" with the punchline \"Nacho cheese!\" is HARMLESS  because This joke is funny because it plays on the phrase \"not your cheese\" which sounds similar to \"nacho cheese\", a type of melted cheese dip popular with tortilla chips. The unexpected pun and cheesy wordplay creates an element of surprise and humor.\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # This allows for nested event loops in Jupyter Notebooks\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str\n",
    "    punchline: str\n",
    "\n",
    "class NsfwGuardrail(BaseModel):\n",
    "    label: Literal[\"HARMLESS\", \"NSFW\"] = Field(\n",
    "        description=\"The predicted class label, output either HARMLESS or NSFW (Not Safe For Work).\",\n",
    "    )\n",
    "\n",
    "class Explainer(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description=\"The explanation why this joke is funny.\"\n",
    "    )\n",
    "\n",
    "joke_agent = Agent(\n",
    "    model='bedrock:anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    deps_type=str,\n",
    "    model_settings=ModelSettings(\n",
    "        temperature=1 # set the temperature to 1 to be more creative, and possibly more funny\n",
    "    ),\n",
    "    system_prompt=(\n",
    "        \"You're a funny joke writer.\"\n",
    "    ),\n",
    "    result_type=Joke\n",
    ")\n",
    "\n",
    "guardrail_agent = Agent(\n",
    "    model='bedrock:anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    deps_type=str,\n",
    "    system_prompt=(\n",
    "        \"You're a moderator ensuring content is safe for work.\"\n",
    "    ),\n",
    "    result_type=NsfwGuardrail\n",
    ")\n",
    "\n",
    "joke_explainer_agent = Agent(\n",
    "    model='bedrock:anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    deps_type=str,\n",
    "    system_prompt=(\n",
    "        \"All you do is explain jokes.\"\n",
    "    ),\n",
    "    result_type=Explainer\n",
    ")\n",
    "\n",
    "result1 = joke_agent.run_sync('Write a joke about cheese.')\n",
    "# Note how we pass the conversation history between PydanticAI agents with the `all_messages()` function\n",
    "result2 = guardrail_agent.run_sync('Judge whether or not this joke is inappropriate.', message_history=result1.all_messages())\n",
    "\n",
    "if result2.data.label == \"HARMLESS\":\n",
    "    # We again pass the message history of the second LLM call, but this will not have \n",
    "    result3 = joke_explainer_agent.run_sync('Why is this Joke funny?', message_history=result1.all_messages())\n",
    "else:\n",
    "    result3 = joke_explainer_agent.run_sync('Why is this Joke inappropriate?', message_history=result1.all_messages())\n",
    "    print(\"Stopped early due to inappropriate joke\")\n",
    "\n",
    "print(\n",
    "    f'The joke \"{result1.data.setup}\" '\n",
    "    f'with the punchline \"{result1.data.punchline}\" '\n",
    "    f'is {result2.data.label} ',\n",
    "    f'because {result3.data.reasoning}',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadaba0f-0da6-4e74-a068-db363bb1997d",
   "metadata": {},
   "source": [
    "# Workflow: Routing\n",
    "\n",
    "Routing is all about classifying an input and directing it to a specialised followup task. This allows separation of concerns to independently optimise different downstream processes. Routing works well when the input distribution can be well seperated into distinct classes.\n",
    "\n",
    "> ![The LLM Router](../public/images/pydantic-ai/router.webp)\n",
    "> Anthropic\n",
    "\n",
    "Let's make a simple model router that directs customer service questions to different specialised agents base on the class of the query. We'll assume the users will either be asking about terms & conditions, refunds or general Q/A.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ebcf1ba-11ce-48fd-a35a-ac068d87ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # This allows for nested event loops in Jupyter Notebooks\n",
    "\n",
    "class QueryClass(BaseModel):\n",
    "    label: Literal[\"REFUND\", \"TERMS_AND_CONDITIONS\"] = Field(\n",
    "        description=\"The predicted class label based on the users question\",\n",
    "    )\n",
    "\n",
    "class AgentResponse(BaseModel):\n",
    "    reasoning: str\n",
    "    final_reply: str\n",
    "\n",
    "router_agent = Agent(\n",
    "    model='bedrock:anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    system_prompt=(\n",
    "        \"You're a query classifier that knows how to take a user question and predict the query intent class.\"\n",
    "    ),\n",
    "    result_type=QueryClass\n",
    ")\n",
    "\n",
    "refund_agent = Agent(\n",
    "    model='bedrock:anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    deps_type=str,\n",
    "    system_prompt=(\n",
    "        \"You're a simulated customer service agent responsible for deciding if customers can have a refund.\"\n",
    "    ),\n",
    "    result_type=AgentResponse\n",
    ")\n",
    "\n",
    "terms_and_conditions_agent = Agent(\n",
    "    model='bedrock:anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    system_prompt=(\n",
    "        \"Reply with simulated generic T&Cs, don't worry if it's accurate, just make stuff up.\"\n",
    "    ),\n",
    "    result_type=AgentResponse\n",
    ")\n",
    "\n",
    "def run_llm_router(q: str):\n",
    "\n",
    "    result1 = router_agent.run_sync(q)\n",
    "    \n",
    "    if result1.data.label == \"REFUND\":\n",
    "        print(\"Model predicted the question was about refunds\")\n",
    "        result2 = refund_agent.run_sync(\"Is this customer eligible for a refund?\", message_history=result1.all_messages())\n",
    "    \n",
    "    if result1.data.label == \"TERMS_AND_CONDITIONS\":\n",
    "        print(\"Model predicted the question was about terms and conditions\")\n",
    "        # Here we might do an information retrieval step with a RAG agent\n",
    "        result2 = terms_and_conditions_agent.run_sync(\"Make up T&Cs relevant to this users question.\", message_history=result1.all_messages())\n",
    "\n",
    "    print(result2.data.final_reply)\n",
    "    print(result2.data.reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "feb7dc49-bcb0-419f-bde1-3c873be1a489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:19:51.573 router_agent run\n",
      "18:19:51.573   preparing model request params\n",
      "18:19:51.573   chat anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Model predicted the question was about refunds\n",
      "18:19:52.846 refund_agent run\n",
      "18:19:52.847   preparing model request params\n",
      "18:19:52.847   chat anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Based on the information provided that your plane was cancelled, you should be eligible for a refund for your ticket. However, I would need some additional details like your airline, flight number, and reason for the cancellation to confirm and process the refund request.\n",
      "Based on the customer's statement that their flight was cancelled, they would likely be eligible for a refund according to most airline policies for cancelled flights that are the airline's fault and not the customer's. More details on the specific situation may be needed to confirm eligibility.\n"
     ]
    }
   ],
   "source": [
    "q = \"Can I have a refund for my ticket? My plane was cancelled.\"\n",
    "run_llm_router(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6ad834a-edf4-404a-8c2d-e7731a99e98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:20:18.400 router_agent run\n",
      "18:20:18.400   preparing model request params\n",
      "18:20:18.401   chat anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Model predicted the question was about terms and conditions\n",
      "18:20:19.780 terms_and_conditions_agent run\n",
      "18:20:19.781   preparing model request params\n",
      "18:20:19.781   chat anthropic.claude-3-sonnet-20240229-v1:0\n",
      "As per most airline policies, you will likely need to pay an overweight baggage fee if your luggage exceeds the maximum weight allowance on your return flight. The specific fees can vary by airline, but they are typically charged per pound/kg that the bag is overweight.\n",
      "The user's query is about having to pay fees if their luggage is overweight on the return portion of their flight. Luggage weight restrictions and associated overweight fees are commonly outlined in an airline's terms and conditions, so classifying this as a TERMS_AND_CONDITIONS query seems appropriate. My response provides a general overview of typical airline policies for overweight baggage fees that is relevant to answering the user's question.\n"
     ]
    }
   ],
   "source": [
    "q = \"Do I need to pay if my luggage is overweight on the return flight?\"\n",
    "run_llm_router(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9926826-a83e-4bb4-8828-1d8c7bf062f2",
   "metadata": {},
   "source": [
    "# Workflow: Parallelization\n",
    "\n",
    "If our task can be split into multiple independent subtasks, we can call miultiple LLMs simultaniouly before aggregating their results. \n",
    "\n",
    "> ![Parallelization](../public/images/pydantic-ai/parallel.webp)\n",
    "> Anthropic - Building effective agents\n",
    "\n",
    "To demonstrate the parallelisation workflow let's create a Python code quality voteing workflow where each judge pays attention to a specific evaluation criteria. We'll create LLM judges that rate code snippets for readability, reusability and maintainability. At then end, we'll aggregate the voats to create the final score.\n",
    "\n",
    "First, let's setup our model and agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e6d157e-cf03-4f38-a94f-27984cf420ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # This allows for nested event loops in Jupyter Notebooks\n",
    "\n",
    "class Readability(BaseModel):\n",
    "    feedback: str\n",
    "    score: Literal[\"GOOD\", \"INTERMEDIATE\", \"POOR\"] = Field(\n",
    "        description=\"How readable is this code to a newcomer to the code base?\",\n",
    "    )\n",
    "\n",
    "class Reusability(BaseModel):\n",
    "    feedback: str\n",
    "    score: Literal[\"GOOD\", \"INTERMEDIATE\", \"POOR\"] = Field(\n",
    "        description=\"How reusable is this code? Is it a GOOD easy to use abstraction or POOR single use code?\",\n",
    "    )\n",
    "\n",
    "class Maintainability(BaseModel):\n",
    "    feedback: str\n",
    "    score: Literal[\"GOOD\", \"INTERMEDIATE\", \"POOR\"] = Field(\n",
    "        description=\"How maintainable is this code? Does it reinvent the wheel or does it make use of well supported libraries?\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96520842-5dfb-492e-8926-d143194979e4",
   "metadata": {},
   "source": [
    "Then we'll define our code quality voteing Agents in a way we can run all three in parallel before aggregating the results. For this we'll leverage python's built in asyncio coroutines but you could use threads or Lambda functions or another fork/exec pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "000f4f25-9513-44bf-9a1d-bf305359a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def run_parralelisation_agent(query: str, result_type: any):\n",
    "    parallelisation_agent = Agent(\n",
    "        model='bedrock:anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "        system_prompt=(\n",
    "            \"You're a helpful Python code quality classifier. Read the supplied code and provide a very short upto 2 sentences of feedback then rate it on the specified criteria.\"\n",
    "        ),\n",
    "        result_type=result_type\n",
    "    )\n",
    "    result = await parallelisation_agent.run(query)\n",
    "    return result\n",
    "\n",
    "def voat_aggregator(quality1, quality2, quality3):\n",
    "   quality_map = {\n",
    "       \"GOOD\": 3,\n",
    "       \"INTERMEDIATE\": 2,\n",
    "       \"POOR\": 1\n",
    "   }\n",
    "   \n",
    "   # Check for invalid inputs\n",
    "   for quality in [quality1, quality2, quality3]:\n",
    "       if quality not in quality_map:\n",
    "           raise ValueError(f\"Invalid quality: {quality}\")\n",
    "   \n",
    "   # Calculate total score (3-9)\n",
    "   score = quality_map[quality1] + quality_map[quality2] + quality_map[quality3]\n",
    "   \n",
    "   return score\n",
    "\n",
    "async def run_parralel_workflow(q: str):\n",
    "    r1, r2, r3 = await asyncio.gather(\n",
    "        run_parralelisation_agent(q, Readability),\n",
    "        run_parralelisation_agent(q, Reusability),\n",
    "        run_parralelisation_agent(q, Maintainability),\n",
    "    )\n",
    "    print(r1.data.feedback)\n",
    "    print(r2.data.feedback)\n",
    "    print(r3.data.feedback)\n",
    "    return voat_aggregator(r1.data.score, r2.data.score, r3.data.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ecc53383-8d97-48c2-9aeb-ab1c73cee95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:21:20.557 parallelisation_agent run\n",
      "18:21:20.558   preparing model request params\n",
      "18:21:20.558   chat anthropic.claude-3-sonnet-20240229-v1:0\n",
      "18:21:20.561 parallelisation_agent run\n",
      "18:21:20.562   preparing model request params\n",
      "18:21:20.562   chat anthropic.claude-3-sonnet-20240229-v1:0\n",
      "18:21:20.565 parallelisation_agent run\n",
      "18:21:20.565   preparing model request params\n",
      "18:21:20.565   chat anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Code is clear and well-documented. Good use of docstrings to explain function behavior.\n",
      "Good abstraction, easy to use random number generator function with clear documentation.\n",
      "Good use of docstrings and function arguments. The function is simple and straightforward.\n",
      "Final aggregated score for q1: 9\n"
     ]
    }
   ],
   "source": [
    "q1 = '''\n",
    "import random\n",
    "\n",
    "def generate_random_number(min_value=0, max_value=100):\n",
    "    \"\"\"\n",
    "    Generate a random integer between min_value and max_value (inclusive).\n",
    "    \n",
    "    Args:\n",
    "        min_value (int): The minimum value of the range (default: 0)\n",
    "        max_value (int): The maximum value of the range (default: 100)\n",
    "        \n",
    "    Returns:\n",
    "        int: A random integer between min_value and max_value\n",
    "    \"\"\"\n",
    "    return random.randint(min_value, max_value)\n",
    "'''\n",
    "\n",
    "final_score = asyncio.run(run_parralel_workflow(q1))\n",
    "print(f\"Final aggregated score for q1: {final_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "322ec05a-f8bd-485f-947e-c2c549e17f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:21:29.597 parallelisation_agent run\n",
      "18:21:29.598   preparing model request params\n",
      "18:21:29.598   chat anthropic.claude-3-sonnet-20240229-v1:0\n",
      "18:21:29.603 parallelisation_agent run\n",
      "18:21:29.603   preparing model request params\n",
      "18:21:29.603   chat anthropic.claude-3-sonnet-20240229-v1:0\n",
      "18:21:29.610 parallelisation_agent run\n",
      "18:21:29.610   preparing model request params\n",
      "18:21:29.611   chat anthropic.claude-3-sonnet-20240229-v1:0\n",
      "The code is a Python function that appears to implement a linear congruential generator for generating pseudorandom numbers. It has some unnecessary complexity.\n",
      "A reusable random number generator function with seed handling. Could use better variable naming.\n",
      "The code implements a linear congruential random number generator. It uses non-standard parameters which may not provide good statistical properties.\n",
      "Final aggregated score for q2: 7\n"
     ]
    }
   ],
   "source": [
    "q2 = '''\n",
    "def my_rand(s=None, a=1664525, c=1013904223, m=2**32, mx=100, mn=0):\n",
    "    global _seed\n",
    "    if not '_seed' in globals():\n",
    "        import time\n",
    "        _seed = int(time.time()) & 0xFFFFFFFF if s is None else s\n",
    "    _seed = (a * _seed + c) % m # lnc generator - dont change!!\n",
    "    r = _seed / m # normalize\n",
    "    return int(mn + r * (mx - mn + 1)) # scale to range\n",
    "\n",
    "'''\n",
    "\n",
    "final_score = asyncio.run(run_parralel_workflow(q2))\n",
    "print(f\"Final aggregated score for q2: {final_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16464fa4-f536-4f7b-a5c7-76fb39b0f07d",
   "metadata": {},
   "source": [
    "Remember that we can of course add tools to any of our agents. Firstly, we might want to actually run the code in an executor! our maintainability judge might call a PyPi API to get upto date information on when a python package was last updated, or the readability agent might run the code through a static code analyser to get specific and uptodate coding standards for our organisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1a314d-56f3-4f5a-b8f0-e2787c3d059b",
   "metadata": {},
   "source": [
    "# Workflow: Evaluator-optimizer\n",
    "In the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop. This workflow is a bit more complex than the previous examples because now we can have loops. \n",
    "\n",
    "> ![Evaluator-optimizer](../public/images/pydantic-ai/eval-opt.webp)\n",
    "> Anthropic - Building effective agents\n",
    "\n",
    "As you'll see below, this fits neatly into the PydanticAI world view of modelling the workflow as a graph.\n",
    "\n",
    "The PydanticAI [Graph](https://ai.pydantic.dev/graph/) has three key components, read up in the docs to learn mode.\n",
    "- Node\n",
    "- End\n",
    "- Graph\n",
    "- GraphRunContext\n",
    "\n",
    "We'll implement an example of two agents working together to write a technical interview question. One agent is the evaluator and one agent is the optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9efbdf2a-647a-4fef-822f-66ec06badb93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:57:22.178 run graph feedback_graph\n",
      "08:57:22.179   run node WriteQuestion\n",
      "08:57:22.180     question_writer_agent run\n",
      "08:57:22.180       preparing model request params\n",
      "08:57:22.180       chat us.anthropic.claude-3-haiku-20240307-v1:0\n",
      "08:57:24.424   run node Feedback\n",
      "08:57:24.425     feedback_agent run\n",
      "08:57:24.426       preparing model request params\n",
      "08:57:24.426       chat anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Question(main_question='Can you describe a time when you had to invent a new solution to a complex problem? How did you go about simplifying the solution to make it more understandable and effective?', follow_up_questions=['How did you identify the need for a new solution?', 'What were the key steps you took to develop and refine the solution?', 'How did you ensure the solution was easy for others to understand and implement?', 'What was the outcome of implementing the new solution, and how did you assess its success?'])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations as _annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.format_as_xml import format_as_xml\n",
    "from pydantic_ai.messages import ModelMessage\n",
    "from pydantic_graph import BaseNode, End, Graph, GraphRunContext\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from IPython.display import Image, display\n",
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # This allows for nested event loops in Jupyter Notebooks\n",
    "\n",
    "@dataclass\n",
    "class LeadershipPrinciple:\n",
    "    name: str\n",
    "    description: str\n",
    "\n",
    "@dataclass\n",
    "class Question:\n",
    "    main_question: str\n",
    "    follow_up_questions: list[str]\n",
    "\n",
    "@dataclass\n",
    "class State:\n",
    "    leadership_principle: LeadershipPrinciple\n",
    "    write_agent_messages: list[ModelMessage] = field(default_factory=list)\n",
    "\n",
    "question_writer_agent = Agent(\n",
    "    model='bedrock:us.anthropic.claude-3-haiku-20240307-v1:0',\n",
    "    result_type=Question,\n",
    "    system_prompt='Write an interview question for a Python software developer role.',\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WriteQuestion(BaseNode[State]):\n",
    "    feedback: str | None = None\n",
    "\n",
    "    async def run(self, ctx: GraphRunContext[State]) -> Feedback:\n",
    "        if self.feedback:\n",
    "            prompt = (\n",
    "                f'Rewrite the question to asses the leadership principle:\\n'\n",
    "                f'{format_as_xml(ctx.state.leadership_principle)}\\n'\n",
    "                f'Feedback: {self.feedback}'\n",
    "            )\n",
    "        else:\n",
    "            prompt = (\n",
    "                f'Write the question to asses the leadership principle:\\n'\n",
    "                f'{format_as_xml(ctx.state.leadership_principle)}\\n'\n",
    "            )\n",
    "\n",
    "        result = await question_writer_agent.run(\n",
    "            prompt,\n",
    "            message_history=ctx.state.write_agent_messages,\n",
    "        )\n",
    "        ctx.state.write_agent_messages += result.all_messages()\n",
    "        return Feedback(result.data)\n",
    "\n",
    "\n",
    "class QuestionRequiresRewrite(BaseModel):\n",
    "    feedback: str\n",
    "\n",
    "\n",
    "class QuestionOk(BaseModel):\n",
    "    pass\n",
    "\n",
    "\n",
    "feedback_agent = Agent[None, QuestionRequiresRewrite | QuestionOk](\n",
    "    model='bedrock:anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    result_type=QuestionRequiresRewrite | QuestionOk,  # type: ignore\n",
    "    system_prompt=(\n",
    "        'Review the interview question and check it assesses the leadership principle, if not, provide feedback.'\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Feedback(BaseNode[State, None, Question]):\n",
    "    question: Question\n",
    "\n",
    "    async def run(\n",
    "        self,\n",
    "        ctx: GraphRunContext[State],\n",
    "    ) -> WriteQuestion | End[Question]:\n",
    "        prompt = format_as_xml({'leadership_principle': ctx.state.leadership_principle, 'interview_question': self.question})\n",
    "        result = await feedback_agent.run(prompt)\n",
    "        if isinstance(result.data, QuestionRequiresRewrite):\n",
    "            return WriteQuestion(feedback=result.data.feedback)\n",
    "        else:\n",
    "            return End(self.question)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    lp = LeadershipPrinciple(\n",
    "        name='Invent and Simplify',\n",
    "        description='Leaders expect and require innovation and invention from their teams and always find ways to simplify. They are externally aware, look for new ideas from everywhere, and are not limited by “not invented here”. As we do new things, we accept that we may be misunderstood for long periods of time.'\n",
    "    )\n",
    "    state = State(lp)\n",
    "    feedback_graph = Graph(nodes=(WriteQuestion, Feedback))\n",
    "    \n",
    "    display(Image(feedback_graph.mermaid_image()))\n",
    "    \n",
    "    result = await feedback_graph.run(WriteQuestion(), state=state)\n",
    "    print(result.output)\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3300d5a-a1c0-47b3-915a-d28c80beeae5",
   "metadata": {},
   "source": [
    "# Workflow: Orchestrator-workers\n",
    "Next, we have the orchestrator-workers workflow. A central LLM dynamically breaks down tasks and delegates these subtasks to worker LLMs.\n",
    "\n",
    "> ![Orchestrator-workers](../public/images/pydantic-ai/orch-work.webp)\n",
    "> Anthropic - Building effective agents\n",
    "\n",
    "Let's take an example from a marketing department in a theatre. In preparation for a new show is announcement a team member needs to reachout to the casts representatives to get headshots. When they are returned, the a design team member needs to reformat them into the correct dimentions required for social media and website platforms. Finally, the socials team should be notified the assets are ready and they should work on the content for the announcement. This process is centrally orchistrated by the marketing manager, let's see if we can automate it with agents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed9d0f61-62f4-48b9-aade-de6fceb660ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations as _annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from pydantic import BaseModel, EmailStr\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.format_as_xml import format_as_xml\n",
    "from pydantic_ai.messages import ModelMessage\n",
    "from pydantic_graph import BaseNode, End, Graph, GraphRunContext\n",
    "\n",
    "from anthropic import AsyncAnthropicBedrock\n",
    "from pydantic_ai.models.anthropic import AnthropicModel\n",
    "from pydantic_ai import Agent\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "from IPython.display import Image, display\n",
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # This allows for nested event loops in Jupyter Notebooks\n",
    "\n",
    "anthropic_bedrock_client = AsyncAnthropicBedrock()\n",
    "\n",
    "@dataclass\n",
    "class ActorCharacter:\n",
    "    actor: str\n",
    "    character: str\n",
    "\n",
    "@dataclass\n",
    "class Show:\n",
    "    title: str\n",
    "    actor_character_map: list[ActorCharacter]\n",
    "\n",
    "@dataclass\n",
    "class State:\n",
    "    show: Show\n",
    "    write_agent_messages: list[ModelMessage] = field(default_factory=list)\n",
    "\n",
    "model = AnthropicModel(\n",
    "    model_name='anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
    "    anthropic_client=anthropic_bedrock_client\n",
    ")\n",
    "\n",
    "@agent.tool\n",
    "def send_email(ctx: RunContext[str]) -> str:\n",
    "    # Use email API like resend, for now just print\n",
    "    # print(ctx.deps)\n",
    "    return 'Success'\n",
    "\n",
    "def mock_receive_email():\n",
    "    return \"\"\"\n",
    "        Hello Peter,\n",
    "        Exciting to hear about the announcement, here's my headshot:\n",
    "\n",
    "        \n",
    "        Let me know if you need anything else,\n",
    "        Regard,\n",
    "        Michelle\n",
    "    \"\"\"\n",
    "\n",
    "actor_liaison_agent = Agent(\n",
    "    model,\n",
    "    result_type=Question,\n",
    "    system_prompt='Your name is Peter. Communicate with actors over email. Use clear and simple language, avoid waffling.',\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class OrchistratorLoop(BaseNode[State]):\n",
    "    feedback: str | None = None\n",
    "\n",
    "    async def run(self, ctx: GraphRunContext[State]) -> Feedback:\n",
    "        if self.feedback:\n",
    "            prompt = (\n",
    "                f'Orchestrate the \\n'\n",
    "                f'{format_as_xml(ctx.state.leadership_principle)}\\n'\n",
    "                f'Feedback: {self.feedback}'\n",
    "            )\n",
    "        else:\n",
    "            prompt = (\n",
    "                f'Write an interview question'\n",
    "            )\n",
    "\n",
    "        result = await question_writer_agent.run(\n",
    "            prompt,\n",
    "            message_history=ctx.state.write_agent_messages,\n",
    "        )\n",
    "        ctx.state.write_agent_messages += result.all_messages()\n",
    "        return Feedback(result.data)\n",
    "\n",
    "\n",
    "class QuestionRequiresWrite(BaseModel):\n",
    "    feedback: str\n",
    "\n",
    "\n",
    "class QuestionOk(BaseModel):\n",
    "    pass\n",
    "\n",
    "\n",
    "photo_editor_agent = Agent[None, QuestionRequiresWrite | QuestionOk](\n",
    "    model,\n",
    "    result_type=QuestionRequiresWrite | QuestionOk,  # type: ignore\n",
    "    system_prompt=(\n",
    "        'Review the interview question and provide feedback, question must assess the leadership principle.'\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Feedback(BaseNode[State, None, Question]):\n",
    "    question: Question\n",
    "\n",
    "    async def run(\n",
    "        self,\n",
    "        ctx: GraphRunContext[State],\n",
    "    ) -> WriteQuestion | End[Question]:\n",
    "        prompt = format_as_xml({'leadership_principle': ctx.state.leadership_principle, 'interview_question': self.question})\n",
    "        result = await feedback_agent.run(prompt)\n",
    "        if isinstance(result.data, QuestionRequiresWrite):\n",
    "            return WriteQuestion(feedback=result.data.feedback)\n",
    "        else:\n",
    "            return End(self.question)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    lp = LeadershipPrinciple(\n",
    "        name='Invent and Simplify',\n",
    "        description='Leaders expect and require innovation and invention from their teams and always find ways to simplify. They are externally aware, look for new ideas from everywhere, and are not limited by “not invented here”. As we do new things, we accept that we may be misunderstood for long periods of time.'\n",
    "    )\n",
    "    state = State(lp)\n",
    "    feedback_graph = Graph(nodes=(WriteQuestion, Feedback))\n",
    "    \n",
    "    display(Image(feedback_graph.mermaid_image()))\n",
    "    \n",
    "    result = await feedback_graph.run(WriteQuestion(), state=state)\n",
    "    print(result.output)\n",
    "\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
