{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "date: \"2025-05-06T15:00:00.00Z\"\n",
    "description: \"A principled approach for evaluating LLM workflows\"\n",
    "published: true\n",
    "tags:\n",
    "  - python\n",
    "  - llm\n",
    "  - ragas\n",
    "  - pydantic-ai\n",
    "  - evals\n",
    "time_to_read: 10\n",
    "title: \"📈 Evaluation Driven Development (EDD) with PydanticAI\"\n",
    "type: post\n",
    "image: \"/public/images/pydantic-ai/evals.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f113c649-3b22-4587-a5f9-8a4b92384f09",
   "metadata": {},
   "source": [
    "\n",
    "When embarking on any AI implementation, I prioritise establishing a robust evaluation framework. The initial development of this evaluation process itself provides valuable insights into effective assessment criteria. Here are the critical aspects I consider when selecting evaluation tools:\n",
    "\n",
    "### Developer Experience\n",
    "The evaluation environment must be optimized for rapid iteration and reliable feedback. An effective framework:\n",
    "\n",
    "- Enables quick, dependable, and observable experiment execution\n",
    "- Helps troubleshoot issues rather than creating additional friction\n",
    "- Simplifies the addition of new test scenarios\n",
    "\n",
    "### Composability\n",
    "Identifying meaningful performance metrics requires deliberate consideration and experimentation. Optimal evaluation might be composed of several metrics including:\n",
    "\n",
    "- Rules based metrics (e.g., BLEU score for NLP tasks)\n",
    "- Reference-free assessments (like LLM-based judges)\n",
    "- Human annotated labels\n",
    "\n",
    "PydanticAI has recently released Evals which provides an interesting and simple approach which may tick a few of my boxes above. Let's try this out with a simple finantial markets text summarisation use-case. We'll be building up the evaluation suite as we go, following the principles of Evaluation Driven Development (EDD)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bfd94f-cade-4cae-8afd-cb39889f55d4",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d925bd1-0530-4661-8c97-e6cc755b1c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!uv pip install --upgrade pydantic-evals 'pydantic-ai-slim[bedrock]' pydantic-graph boto3 logfire"
   ]
  },
  {
   "cell_type": "code",
   "id": "dccbb17f-9ece-46ad-973c-3173f247b5e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T09:56:37.062590Z",
     "start_time": "2025-05-13T09:56:34.851415Z"
    }
   },
   "source": [
    "%%capture\n",
    "!logfire auth\n",
    "!logfire projects use stephenhib-blog\n",
    "\n",
    "import logfire\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "logfire.configure(send_to_logfire='if-token-present', scrubbing=False)\n",
    "Agent.instrument_all()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "2a100bfc-ca02-4721-b733-35577d071d8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T09:56:50.813800Z",
     "start_time": "2025-05-13T09:56:49.320146Z"
    }
   },
   "source": [
    "from pydantic_evals import Case, Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def convert_hf_to_pydantic_dataset(\n",
    "    hf_dataset_name,\n",
    "    input_column=\"user_input\",\n",
    "    split=\"train\",\n",
    "    subset=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert a Hugging Face dataset to a PydanticAI Dataset\n",
    "    \n",
    "    Args:\n",
    "        hf_dataset_name: Name of the Hugging Face dataset\n",
    "        input_column: Column to use as Case inputs\n",
    "        output_column: Column to use as model outputs (if available)\n",
    "        split: Dataset split to use\n",
    "    \n",
    "    Returns:\n",
    "        A Pydantic Dataset object\n",
    "    \"\"\"\n",
    "    # Load the Hugging Face dataset\n",
    "    hf_dataset = load_dataset(hf_dataset_name, split=split)\n",
    "    \n",
    "    # Convert each row to a Pydantic Case\n",
    "    cases = []\n",
    "    for i, item in enumerate(hf_dataset):\n",
    "        # Create a case name using the index\n",
    "        case_name = f\"case_{i}\"\n",
    "        \n",
    "        # Extract the required fields\n",
    "        case_input = item.get(input_column)\n",
    "        \n",
    "        # Create the case\n",
    "        case = Case(\n",
    "            name=case_name,\n",
    "            inputs=case_input,\n",
    "            expected_output=None, # No expected output, we'll let the LLM judge the quality.\n",
    "        )\n",
    "        cases.append(case)\n",
    "    \n",
    "    # Create and return the Dataset\n",
    "    return Dataset(cases=cases)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "90de7c6a-986c-4015-989a-dfe9d8837be7",
   "metadata": {},
   "source": [
    "We'll use the small `explodinggradients/earning_report_summary` dataset downloaded from HuggingFace to get started. We use the helper function above to convert it into the input format PydanticAI expects."
   ]
  },
  {
   "cell_type": "code",
   "id": "1fffd51b-3605-40d3-b7a4-117c1b7eb784",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:45:26.086724Z",
     "start_time": "2025-05-13T10:45:24.845051Z"
    }
   },
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import asyncio\n",
    "from pydantic_ai import Agent, format_as_xml\n",
    "from pydantic_evals import Case, Dataset\n",
    "from pydantic_evals.evaluators import IsInstance, LLMJudge, Evaluator, EvaluatorContext\n",
    "\n",
    "dataset = convert_hf_to_pydantic_dataset(\n",
    "    \"explodinggradients/earning_report_summary\", \n",
    "    input_column=\"user_input\",\n",
    "    split=\"train[0:10]\", # Load the first 10 rows from the train split\n",
    ")\n",
    "\n",
    "print(f\"Created Pydantic Dataset with {len(dataset.cases)} cases\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Pydantic Dataset with 10 cases\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "45640c2b-6791-4889-be36-c090d10e50b6",
   "metadata": {},
   "source": [
    "Now, let's implement our first evaluator. This uses the build-in `LLMJudge` with a rubric to check for hullucination by checking that any facts in the output are explicitly mentioned in the input."
   ]
  },
  {
   "cell_type": "code",
   "id": "ac02170f-aff3-4472-9945-a37cc288dff2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:45:27.845424Z",
     "start_time": "2025-05-13T10:45:27.842900Z"
    }
   },
   "source": [
    "dataset.add_evaluator(\n",
    "    LLMJudge(\n",
    "            rubric='The summary must not contain any financial advice',\n",
    "            include_input=True,\n",
    "            model='bedrock:us.anthropic.claude-3-7-sonnet-20250219-v1:0', # It's good practice to use a large, capable model for LLM Judge\n",
    "        )\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "c76daa41-f1a8-488d-85c5-c9f5af448909",
   "metadata": {},
   "source": [
    "When implementing evals, I like to do an inital run with a bad output to see the test fails and my framework works before implementing anything more sopfisticated. This is very similar to Test Driven Development (TDD) practice, but here we are doing Eval Driven Development (EDD). This helps iron out any testing issues before working on the implementation. Let's start by simply predicting the same answer for each row in our evaluaiton dataset:"
   ]
  },
  {
   "cell_type": "code",
   "id": "46fc3c03-ff88-422a-9324-4755ac1991da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:00:03.647131Z",
     "start_time": "2025-05-13T09:58:30.708793Z"
    }
   },
   "source": [
    "async def generate_bad_summary(news_inputs: str) -> str:\n",
    "    await asyncio.sleep(5) # Trying hard to avoid throttle limits\n",
    "    return 'Sell everything and invest in crypto!' # Always return the same hardcoded string - clearly a terrible summary!\n",
    "    \n",
    "report = dataset.evaluate_sync(generate_bad_summary, max_concurrency=1) # Limit concurrency to avoid throttle limits\n",
    "print(report)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[3m        Evaluation Summary:         \u001B[0m\n",
       "\u001B[3m        generate_bad_summary        \u001B[0m\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mCase ID \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mAssertions\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mDuration\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_0  \u001B[0m\u001B[1m \u001B[0m│ \u001B[31m✗\u001B[0m          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_1  \u001B[0m\u001B[1m \u001B[0m│ \u001B[31m✗\u001B[0m          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_2  \u001B[0m\u001B[1m \u001B[0m│ \u001B[31m✗\u001B[0m          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_3  \u001B[0m\u001B[1m \u001B[0m│ \u001B[31m✗\u001B[0m          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_4  \u001B[0m\u001B[1m \u001B[0m│ \u001B[31m✗\u001B[0m          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_5  \u001B[0m\u001B[1m \u001B[0m│ \u001B[31m✗\u001B[0m          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_6  \u001B[0m\u001B[1m \u001B[0m│ \u001B[31m✗\u001B[0m          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_7  \u001B[0m\u001B[1m \u001B[0m│ \u001B[31m✗\u001B[0m          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_8  \u001B[0m\u001B[1m \u001B[0m│ \u001B[31m✗\u001B[0m          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_9  \u001B[0m\u001B[1m \u001B[0m│ \u001B[31m✗\u001B[0m          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1;3mAverages\u001B[0m\u001B[1m \u001B[0m│ 0.0% \u001B[32m✔\u001B[0m     │     5.0s │\n",
       "└──────────┴────────────┴──────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">        Evaluation Summary:         </span>\n",
       "<span style=\"font-style: italic\">        generate_bad_summary        </span>\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Case ID  </span>┃<span style=\"font-weight: bold\"> Assertions </span>┃<span style=\"font-weight: bold\"> Duration </span>┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"font-weight: bold\"> case_0   </span>│ <span style=\"color: #800000; text-decoration-color: #800000\">✗</span>          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_1   </span>│ <span style=\"color: #800000; text-decoration-color: #800000\">✗</span>          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_2   </span>│ <span style=\"color: #800000; text-decoration-color: #800000\">✗</span>          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_3   </span>│ <span style=\"color: #800000; text-decoration-color: #800000\">✗</span>          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_4   </span>│ <span style=\"color: #800000; text-decoration-color: #800000\">✗</span>          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_5   </span>│ <span style=\"color: #800000; text-decoration-color: #800000\">✗</span>          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_6   </span>│ <span style=\"color: #800000; text-decoration-color: #800000\">✗</span>          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_7   </span>│ <span style=\"color: #800000; text-decoration-color: #800000\">✗</span>          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_8   </span>│ <span style=\"color: #800000; text-decoration-color: #800000\">✗</span>          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_9   </span>│ <span style=\"color: #800000; text-decoration-color: #800000\">✗</span>          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> </span><span style=\"font-weight: bold; font-style: italic\">Averages</span><span style=\"font-weight: bold\"> </span>│ 0.0% <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>     │     5.0s │\n",
       "└──────────┴────────────┴──────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "34080186-5f5d-41d2-9491-7119c9efa61e",
   "metadata": {},
   "source": "As expected, performance is poor. Next, we'll implement something more sophisticated. An AI summariser that uses an LLM to create a structured output defined by the `Summary` Pydantic model, following the instructions in the system prompt to `Create a short, concise summary of the news.`."
  },
  {
   "cell_type": "code",
   "id": "30b7def3-f42d-4844-8ef6-bbf66d45eb34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:04:24.871863Z",
     "start_time": "2025-05-13T10:01:18.332360Z"
    }
   },
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    title: str\n",
    "    facts: list[str]\n",
    "    summary: str\n",
    "\n",
    "summary_agent = Agent(\n",
    "    'bedrock:us.anthropic.claude-3-5-haiku-20241022-v1:0', # Using a smaller, faster, cheaper but also less capable model\n",
    "    output_type=Summary,\n",
    "    system_prompt = (f\"\"\"Create a short, concise summary of the news article that prioritizes factual accuracy.\n",
    "Follow these guidelines:\n",
    "- Present only verifiable facts from the original text\n",
    "- Maintain the original meaning without distortion\n",
    "- Make sure that the summary does not contain any financial advice\n",
    "\n",
    "Respond with a structured output according to {Summary.model_json_schema()}\"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "async def generate_better_summary(news_inputs: str) -> Summary:\n",
    "    await asyncio.sleep(10) # Trying even harder to avoid throttle limits\n",
    "    r = await summary_agent.run({format_as_xml(news_inputs)})\n",
    "    return r.output.summary\n",
    "\n",
    "report = dataset.evaluate_sync(generate_better_summary, max_concurrency=1)\n",
    "print(report)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[3m        Evaluation Summary: generate_better_summary         \u001B[0m\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mCase ID \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mMetrics              \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mAssertions\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mDuration\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_0  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1434    │ \u001B[32m✔\u001B[0m          │    13.3s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 324    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_1  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1410    │ \u001B[32m✔\u001B[0m          │    13.5s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 324    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_2  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1444    │ \u001B[32m✔\u001B[0m          │    13.7s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 306    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_3  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1454    │ \u001B[32m✔\u001B[0m          │    17.8s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 336    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_4  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1442    │ \u001B[32m✔\u001B[0m          │    13.0s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 346    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_5  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1430    │ \u001B[32m✔\u001B[0m          │    13.5s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 358    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_6  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1426    │ \u001B[32m✔\u001B[0m          │    12.5s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 302    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_7  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1444    │ \u001B[32m✔\u001B[0m          │    13.3s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 346    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_8  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1436    │ \u001B[32m✔\u001B[0m          │    13.0s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 370    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_9  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1422    │ \u001B[32m✔\u001B[0m          │    16.9s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 360    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1;3mAverages\u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1,434.2 │ 100.0% \u001B[32m✔\u001B[0m   │    14.0s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 337.2  │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1.00        │            │          │\n",
       "└──────────┴───────────────────────┴────────────┴──────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">        Evaluation Summary: generate_better_summary         </span>\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Case ID  </span>┃<span style=\"font-weight: bold\"> Metrics               </span>┃<span style=\"font-weight: bold\"> Assertions </span>┃<span style=\"font-weight: bold\"> Duration </span>┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"font-weight: bold\"> case_0   </span>│ input_tokens: 1434    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>          │    13.3s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 324    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_1   </span>│ input_tokens: 1410    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>          │    13.5s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 324    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_2   </span>│ input_tokens: 1444    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>          │    13.7s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 306    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_3   </span>│ input_tokens: 1454    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>          │    17.8s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 336    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_4   </span>│ input_tokens: 1442    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>          │    13.0s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 346    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_5   </span>│ input_tokens: 1430    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>          │    13.5s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 358    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_6   </span>│ input_tokens: 1426    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>          │    12.5s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 302    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_7   </span>│ input_tokens: 1444    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>          │    13.3s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 346    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_8   </span>│ input_tokens: 1436    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>          │    13.0s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 370    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_9   </span>│ input_tokens: 1422    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>          │    16.9s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 360    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> </span><span style=\"font-weight: bold; font-style: italic\">Averages</span><span style=\"font-weight: bold\"> </span>│ input_tokens: 1,434.2 │ 100.0% <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>   │    14.0s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 337.2  │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1.00        │            │          │\n",
       "└──────────┴───────────────────────┴────────────┴──────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "2a75cabb-cc51-4d3a-ab82-139f200568f7",
   "metadata": {},
   "source": "Now, we have better performance on the financial advice rubric, at least according to our evaluation dataset. At this point, it's really important to spend some time looking at inputs and outputs. Remember, the evaluations won't improve your application by themselves. It takes you, the AI engineer to interpret the shortcomings in the system and use that insight to improve the evaluations and the application. I strongly recommend using a developer friendly observability platform to do this, especially for more complex LLM workflows. But for now, here's the first row printed out:\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:50:18.305385Z",
     "start_time": "2025-05-13T10:50:18.303050Z"
    }
   },
   "cell_type": "code",
   "source": "report.cases[9]",
   "id": "923a0aa146c1b2dd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReportCase(name='case_9', inputs='summarise given text\\nIn 2023, the company experienced a 5% decline in its market share, raising concerns among investors. Analysts attributed this drop to increased competition and changing consumer preferences. Despite the decline, the company is implementing new strategies to regain its market position. The management remains optimistic about reversing the trend by the end of the year.', metadata=None, expected_output=None, output=\"In 2023, the company faced a 5% market share reduction due to competitive pressures and shifting consumer demands. Responding proactively, the leadership is developing strategies aimed at improving the company's market position and expressing confidence in potential recovery.\", metrics={'input_tokens': 1422, 'output_tokens': 352, 'requests': 1}, attributes={}, scores={}, labels={}, assertions={'LLMJudge': EvaluationResult(name='LLMJudge', value=True, reason=\"The summary contains only factual information about the company's market position, the reasons for decline, and their planned response. It does not offer any financial advice such as recommendations to buy, sell, or hold stock, or make any claims about investment potential.\", source=LLMJudge(rubric='The summary must not contain any financial advice', model='bedrock:us.anthropic.claude-3-7-sonnet-20250219-v1:0', include_input=True))}, task_duration=13.064025, total_duration=21.591622, trace_id='0196c940a70ce23393c4fab4ea7c2c50', span_id='b7cd1a61b46f3416')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's move onto evaluating and optimising another characteristic of our system. Imagine we are building a mobile app page where we can only show a maximum of 250 characters in the summary field. So, following our EDD principles, we can add another `Evaluator` which is a test for summary length:",
   "id": "127baa47dfbe5cc9"
  },
  {
   "cell_type": "code",
   "id": "fb89b76a-31f7-49d1-ab0b-c6afce05e5a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:07:31.310609Z",
     "start_time": "2025-05-13T10:07:31.307824Z"
    }
   },
   "source": [
    "@dataclass\n",
    "class SummaryLengthEvaluator(Evaluator):\n",
    "    max_num_chars: int\n",
    "    async def evaluate(self, ctx: EvaluatorContext[str, str]) -> bool:  \n",
    "        if len(ctx.output) <= self.max_num_chars:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "dataset.add_evaluator(SummaryLengthEvaluator(250)) # Summary should be 250 characters or fewer"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "db07ba2f-0bb3-4ee8-a694-ed6625a89108",
   "metadata": {},
   "source": [
    "Ideally, we add this evaluation **before** changing our prompt to provide instructions for this part of the task. This approach creates the baseline first, and makes it easier to identify any areas of performance trade-offs. Eventually, our prompt may be trying to achieve many things, some of which may interact. The evaluations will help spot regressions that could creep in by optimising for one area over another."
   ]
  },
  {
   "cell_type": "code",
   "id": "7eebe1d2-b98b-46cf-b6f8-b70832daed9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:10:43.557533Z",
     "start_time": "2025-05-13T10:07:40.496594Z"
    }
   },
   "source": [
    "# Simply re-running the same generation, but now with more evals\n",
    "report = dataset.evaluate_sync(generate_better_summary, max_concurrency=1)\n",
    "print(report)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[3m        Evaluation Summary: generate_better_summary         \u001B[0m\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mCase ID \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mMetrics              \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mAssertions\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mDuration\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_0  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1434    │ \u001B[32m✔\u001B[0m\u001B[31m✗\u001B[0m         │    14.6s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 314    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_1  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1410    │ \u001B[32m✔\u001B[0m\u001B[31m✗\u001B[0m         │    13.1s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 332    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_2  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1444    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    13.7s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 302    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_3  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1454    │ \u001B[32m✔\u001B[0m\u001B[31m✗\u001B[0m         │    14.6s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 336    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_4  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1442    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    13.4s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 338    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_5  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1430    │ \u001B[32m✔\u001B[0m\u001B[31m✗\u001B[0m         │    18.5s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 360    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_6  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1426    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    13.6s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 286    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_7  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1444    │ \u001B[32m✔\u001B[0m\u001B[31m✗\u001B[0m         │    13.3s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 332    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_8  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1436    │ \u001B[32m✔\u001B[0m\u001B[31m✗\u001B[0m         │    14.0s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 366    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_9  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1422    │ \u001B[32m✔\u001B[0m\u001B[31m✗\u001B[0m         │    13.3s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 360    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1;3mAverages\u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1,434.2 │ 65.0% \u001B[32m✔\u001B[0m    │    14.2s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 332.6  │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1.00        │            │          │\n",
       "└──────────┴───────────────────────┴────────────┴──────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">        Evaluation Summary: generate_better_summary         </span>\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Case ID  </span>┃<span style=\"font-weight: bold\"> Metrics               </span>┃<span style=\"font-weight: bold\"> Assertions </span>┃<span style=\"font-weight: bold\"> Duration </span>┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"font-weight: bold\"> case_0   </span>│ input_tokens: 1434    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span><span style=\"color: #800000; text-decoration-color: #800000\">✗</span>         │    14.6s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 314    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_1   </span>│ input_tokens: 1410    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span><span style=\"color: #800000; text-decoration-color: #800000\">✗</span>         │    13.1s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 332    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_2   </span>│ input_tokens: 1444    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    13.7s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 302    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_3   </span>│ input_tokens: 1454    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span><span style=\"color: #800000; text-decoration-color: #800000\">✗</span>         │    14.6s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 336    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_4   </span>│ input_tokens: 1442    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    13.4s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 338    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_5   </span>│ input_tokens: 1430    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span><span style=\"color: #800000; text-decoration-color: #800000\">✗</span>         │    18.5s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 360    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_6   </span>│ input_tokens: 1426    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    13.6s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 286    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_7   </span>│ input_tokens: 1444    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span><span style=\"color: #800000; text-decoration-color: #800000\">✗</span>         │    13.3s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 332    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_8   </span>│ input_tokens: 1436    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span><span style=\"color: #800000; text-decoration-color: #800000\">✗</span>         │    14.0s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 366    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_9   </span>│ input_tokens: 1422    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span><span style=\"color: #800000; text-decoration-color: #800000\">✗</span>         │    13.3s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 360    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> </span><span style=\"font-weight: bold; font-style: italic\">Averages</span><span style=\"font-weight: bold\"> </span>│ input_tokens: 1,434.2 │ 65.0% <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>    │    14.2s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 332.6  │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1.00        │            │          │\n",
       "└──────────┴───────────────────────┴────────────┴──────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "89147187-99b9-43a3-86a2-f0203109c28e",
   "metadata": {},
   "source": "Now, we can re-write our prompt to try and reduce the length of the summaries:"
  },
  {
   "cell_type": "code",
   "id": "5efd39b8-f277-43a5-8bdb-9c22d4ede791",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:20:32.538863Z",
     "start_time": "2025-05-13T10:15:58.092733Z"
    }
   },
   "source": [
    "concise_summary_agent = Agent(\n",
    "    'bedrock:us.anthropic.claude-3-5-sonnet-20241022-v2:0', # Sonnet 3.5 is a very capable mid-sized model\n",
    "    output_type=Summary,\n",
    "    system_prompt = (f\"\"\"Create a short, concise summary of the news article that prioritizes factual accuracy.\n",
    "Follow these guidelines:\n",
    "- Present only verifiable facts from the original text\n",
    "- Maintain the original meaning without distortion\n",
    "- Make sure that the summary does not contain any financial advice\n",
    "- Make sure the summary is short and has fewer than 250 characters (about 30 words)\n",
    "\n",
    "Respond with a structured output according to {Summary.model_json_schema()}\"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "async def generate_concise_summary(news_inputs: str) -> Summary:\n",
    "    await asyncio.sleep(15) # Trying even harder to avoid throttle limits, longer sleep because of the shorter generation\n",
    "    r = await concise_summary_agent.run({format_as_xml(news_inputs)}, instrument=True)\n",
    "    return r.output.summary\n",
    "\n",
    "report = dataset.evaluate_sync(generate_concise_summary, max_concurrency=1)\n",
    "print(report)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[3m        Evaluation Summary: generate_concise_summary        \u001B[0m\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mCase ID \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mMetrics              \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mAssertions\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mDuration\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_0  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1398    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    18.7s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 248    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_1  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1374    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    18.7s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 298    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_2  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1408    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    18.0s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 278    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_3  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1418    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    19.8s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 344    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_4  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1406    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    30.3s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 284    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_5  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1394    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    22.6s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 312    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_6  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1390    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    24.0s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 262    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_7  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1408    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    24.4s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 292    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_8  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1400    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    28.0s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 314    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_9  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1386    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    23.6s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 282    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1;3mAverages\u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1,398.2 │ 100.0% \u001B[32m✔\u001B[0m   │    22.8s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 291.4  │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1.00        │            │          │\n",
       "└──────────┴───────────────────────┴────────────┴──────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">        Evaluation Summary: generate_concise_summary        </span>\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Case ID  </span>┃<span style=\"font-weight: bold\"> Metrics               </span>┃<span style=\"font-weight: bold\"> Assertions </span>┃<span style=\"font-weight: bold\"> Duration </span>┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"font-weight: bold\"> case_0   </span>│ input_tokens: 1398    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    18.7s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 248    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_1   </span>│ input_tokens: 1374    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    18.7s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 298    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_2   </span>│ input_tokens: 1408    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    18.0s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 278    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_3   </span>│ input_tokens: 1418    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    19.8s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 344    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_4   </span>│ input_tokens: 1406    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    30.3s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 284    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_5   </span>│ input_tokens: 1394    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    22.6s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 312    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_6   </span>│ input_tokens: 1390    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    24.0s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 262    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_7   </span>│ input_tokens: 1408    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    24.4s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 292    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_8   </span>│ input_tokens: 1400    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    28.0s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 314    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_9   </span>│ input_tokens: 1386    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    23.6s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 282    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> </span><span style=\"font-weight: bold; font-style: italic\">Averages</span><span style=\"font-weight: bold\"> </span>│ input_tokens: 1,398.2 │ 100.0% <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>   │    22.8s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 291.4  │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1.00        │            │          │\n",
       "└──────────┴───────────────────────┴────────────┴──────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "d4fb3159-734a-49cf-85a0-7ac7b92780b7",
   "metadata": {},
   "source": "Now the tests pass! But, our application certainly isn't perfect. We could go on to ask if there's now missing important information that we're not capturing in the summary because it's too short, or maybe the tone of the summary isn't in line with brand guidelines. Once we have some curated good examples of summaries, we can add these to the prompt as few shot examples. We should also try to increase the number of rows in the evaluation dataset (ideally hard examples from the true input distribution) to get a higher degree of statistical significance. The process of engineering the evals goes on!"
  },
  {
   "cell_type": "markdown",
   "id": "59adc58a-7a77-43ea-a227-6d3a3e59d0a0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Today you've seen a practical example of how to practice Eval Driven Development (EDD) for a financial news summarisation use-case using the simple PydanticAI Evals framework. The key takeaway is how you can use a principled, structured methodology to building and evaluating with LLMs. I hope it's helped demystify this area and given you confidence to implement evals earlier in your AI application dev cycle. Please do get in touch if you agree or disagree with this approach to EDD. Happy building!"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d9662c08660e550c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
