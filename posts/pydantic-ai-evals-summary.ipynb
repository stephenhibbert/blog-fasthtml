{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "date: \"2025-05-06T15:00:00.00Z\"\n",
    "description: \"A Principled Approach for Evaluating Summarisation Tasks\"\n",
    "published: true\n",
    "tags:\n",
    "  - python\n",
    "  - llm\n",
    "  - ragas\n",
    "  - pydantic-ai\n",
    "  - evals\n",
    "time_to_read: 10\n",
    "title: \"📈 Evaluation Driven Development (EDD) with PydanticAI\"\n",
    "type: post\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f113c649-3b22-4587-a5f9-8a4b92384f09",
   "metadata": {},
   "source": [
    "\n",
    "When embarking on any AI implementation, I prioritise establishing a robust evaluation framework. The initial development of this evaluation process itself provides valuable insights into effective assessment criteria. Here are the critical aspects I consider when selecting evaluation tools:\n",
    "\n",
    "### Developer Experience\n",
    "The evaluation environment must be optimized for rapid iteration and reliable feedback. An effective framework:\n",
    "\n",
    "- Enables quick, dependable, and observable experiment execution\n",
    "- Helps troubleshoot issues rather than creating additional friction\n",
    "- Simplifies the addition of new test scenarios\n",
    "\n",
    "### Composability\n",
    "Identifying meaningful performance metrics requires deliberate consideration and experimentation. Optimal evaluation might be composed of several metrics including:\n",
    "\n",
    "- Rules based metrics (e.g., BLEU score for NLP tasks)\n",
    "- Reference-free assessments (like LLM-based judges)\n",
    "- Human annotated labels\n",
    "\n",
    "PydanticAI has recently released Evals which provides an interesting and simple approach which may tick a few of my boxes above. Let's try this out with a simple finantial markets text summarisation use-case. We'll be building up the evaluation suite as we go, following the principles of Evaluation Driven Development (EDD)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bfd94f-cade-4cae-8afd-cb39889f55d4",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d925bd1-0530-4661-8c97-e6cc755b1c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!uv pip install --upgrade pydantic-evals 'pydantic-ai-slim[bedrock]' pydantic-graph boto3 logfire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccbb17f-9ece-46ad-973c-3173f247b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!logfire auth\n",
    "!logfire projects use stephenhib-blog\n",
    "\n",
    "import logfire\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "logfire.configure(send_to_logfire='if-token-present', scrubbing=False)\n",
    "Agent.instrument_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a100bfc-ca02-4721-b733-35577d071d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Logfire</span> project URL: <a href=\"https://logfire-us.pydantic.dev/stephenhibbert/coffee\" target=\"_blank\"><span style=\"color: #008080; text-decoration-color: #008080; text-decoration: underline\">https://logfire-us.pydantic.dev/stephenhibbert/coffee</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1mLogfire\u001B[0m project URL: \u001B]8;id=812658;https://logfire-us.pydantic.dev/stephenhibbert/coffee\u001B\\\u001B[4;36mhttps://logfire-us.pydantic.dev/stephenhibbert/coffee\u001B[0m\u001B]8;;\u001B\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pydantic_evals import Case, Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def convert_hf_to_pydantic_dataset(\n",
    "    hf_dataset_name,\n",
    "    input_column=\"user_input\",\n",
    "    split=\"train\",\n",
    "    subset=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert a Hugging Face dataset to a PydanticAI Dataset\n",
    "    \n",
    "    Args:\n",
    "        hf_dataset_name: Name of the Hugging Face dataset\n",
    "        input_column: Column to use as Case inputs\n",
    "        output_column: Column to use as model outputs (if available)\n",
    "        split: Dataset split to use\n",
    "    \n",
    "    Returns:\n",
    "        A Pydantic Dataset object\n",
    "    \"\"\"\n",
    "    # Load the Hugging Face dataset\n",
    "    hf_dataset = load_dataset(hf_dataset_name, split=split)\n",
    "    \n",
    "    # Convert each row to a Pydantic Case\n",
    "    cases = []\n",
    "    for i, item in enumerate(hf_dataset):\n",
    "        # Create a case name using the index\n",
    "        case_name = f\"case_{i}\"\n",
    "        \n",
    "        # Extract the required fields\n",
    "        case_input = item.get(input_column)\n",
    "        \n",
    "        # Create the case\n",
    "        case = Case(\n",
    "            name=case_name,\n",
    "            inputs=case_input,\n",
    "            expected_output=None, # No expected output, we'll let the LLM judge the quality.\n",
    "        )\n",
    "        cases.append(case)\n",
    "    \n",
    "    # Create and return the Dataset\n",
    "    return Dataset(cases=cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de7c6a-986c-4015-989a-dfe9d8837be7",
   "metadata": {},
   "source": [
    "We'll use the small `explodinggradients/earning_report_summary` dataset downloaded from HuggingFace to get started. We use the helper function above to convert it into the input format PydanticAI expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fffd51b-3605-40d3-b7a4-117c1b7eb784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Pydantic Dataset with 10 cases\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import asyncio\n",
    "from pydantic_ai import Agent, format_as_xml\n",
    "from pydantic_evals import Case, Dataset\n",
    "from pydantic_evals.evaluators import IsInstance, LLMJudge, Evaluator, EvaluatorContext\n",
    "\n",
    "dataset = convert_hf_to_pydantic_dataset(\n",
    "    \"explodinggradients/earning_report_summary\", \n",
    "    input_column=\"user_input\",\n",
    "    split=\"train[0:10]\", # Load the first 10 rows from the train split\n",
    ")\n",
    "\n",
    "print(f\"Created Pydantic Dataset with {len(dataset.cases)} cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45640c2b-6791-4889-be36-c090d10e50b6",
   "metadata": {},
   "source": [
    "Now, let's implement our first evaluator. This uses the build-in `LLMJudge` with a rubric to check for hullucination by checking that any facts in the output are explicitly mentioned in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac02170f-aff3-4472-9945-a37cc288dff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.add_evaluator(\n",
    "    LLMJudge(\n",
    "            rubric='All facts in the output are correct and explicitly present in the input',\n",
    "            include_input=True,\n",
    "            model='bedrock:us.anthropic.claude-3-7-sonnet-20250219-v1:0', # It's good practice to use a large, capable model for LLM Judge\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76daa41-f1a8-488d-85c5-c9f5af448909",
   "metadata": {},
   "source": [
    "When implementing evals, I like to do an inital run with a bad output to see the test fails and my framework works before implementing anything more sopfisticated. This is very similar to Test Driven Development (TDD) practice, but here we are doing Eval Driven Development (EDD). This helps iron out any testing issues before working on the implementation. Let's start by simply predicting the same answer for each row in our evaluaiton dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46fc3c03-ff88-422a-9324-4755ac1991da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06:00:05.476 evaluate generate_bad_summary\n",
      "06:00:05.478   case: case_0\n",
      "06:00:05.479     execute generate_bad_summary\n",
      "06:00:12.098     judge_input_output run\n",
      "06:00:12.098       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:00:15.421   case: case_1\n",
      "06:00:15.422     execute generate_bad_summary\n",
      "06:00:20.429     judge_input_output run\n",
      "06:00:20.430       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:00:28.539   case: case_2\n",
      "06:00:28.539     execute generate_bad_summary\n",
      "06:00:33.546     judge_input_output run\n",
      "06:00:33.546       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:00:38.097   case: case_3\n",
      "06:00:38.098     execute generate_bad_summary\n",
      "06:00:43.107     judge_input_output run\n",
      "06:00:43.108       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:00:46.327   case: case_4\n",
      "06:00:46.327     execute generate_bad_summary\n",
      "06:00:51.334     judge_input_output run\n",
      "06:00:51.335       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:01:00.734   case: case_5\n",
      "06:01:00.734     execute generate_bad_summary\n",
      "06:01:05.741     judge_input_output run\n",
      "06:01:05.742       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:01:09.316   case: case_6\n",
      "06:01:09.317     execute generate_bad_summary\n",
      "06:01:14.324     judge_input_output run\n",
      "06:01:14.325       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:01:17.731   case: case_7\n",
      "06:01:17.732     execute generate_bad_summary\n",
      "06:01:22.840     judge_input_output run\n",
      "06:01:22.840       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:01:26.157   case: case_8\n",
      "06:01:26.157     execute generate_bad_summary\n",
      "06:01:31.166     judge_input_output run\n",
      "06:01:31.167       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:01:34.829   case: case_9\n",
      "06:01:34.829     execute generate_bad_summary\n",
      "06:01:39.838     judge_input_output run\n",
      "06:01:39.839       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">        Evaluation Summary:         </span>\n",
       "<span style=\"font-style: italic\">        generate_bad_summary        </span>\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Case ID  </span>┃<span style=\"font-weight: bold\"> Assertions </span>┃<span style=\"font-weight: bold\"> Duration </span>┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"font-weight: bold\"> case_0   </span>│ <span style=\"color: #800000; text-decoration-color: #800000\">✗</span>          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_1   </span>│ <span style=\"color: #800000; text-decoration-color: #800000\">✗</span>          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_2   </span>│ <span style=\"color: #800000; text-decoration-color: #800000\">✗</span>          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_3   </span>│ <span style=\"color: #800000; text-decoration-color: #800000\">✗</span>          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_4   </span>│ <span style=\"color: #800000; text-decoration-color: #800000\">✗</span>          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_5   </span>│ <span style=\"color: #800000; text-decoration-color: #800000\">✗</span>          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_6   </span>│ <span style=\"color: #800000; text-decoration-color: #800000\">✗</span>          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_7   </span>│ <span style=\"color: #800000; text-decoration-color: #800000\">✗</span>          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_8   </span>│ <span style=\"color: #800000; text-decoration-color: #800000\">✗</span>          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_9   </span>│ <span style=\"color: #800000; text-decoration-color: #800000\">✗</span>          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> </span><span style=\"font-weight: bold; font-style: italic\">Averages</span><span style=\"font-weight: bold\"> </span>│ 0.0% <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>     │     5.0s │\n",
       "└──────────┴────────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[3m        Evaluation Summary:         \u001B[0m\n",
       "\u001B[3m        generate_bad_summary        \u001B[0m\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mCase ID \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mAssertions\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mDuration\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_0  \u001B[0m\u001B[1m \u001B[0m│ \u001B[31m✗\u001B[0m          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_1  \u001B[0m\u001B[1m \u001B[0m│ \u001B[31m✗\u001B[0m          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_2  \u001B[0m\u001B[1m \u001B[0m│ \u001B[31m✗\u001B[0m          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_3  \u001B[0m\u001B[1m \u001B[0m│ \u001B[31m✗\u001B[0m          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_4  \u001B[0m\u001B[1m \u001B[0m│ \u001B[31m✗\u001B[0m          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_5  \u001B[0m\u001B[1m \u001B[0m│ \u001B[31m✗\u001B[0m          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_6  \u001B[0m\u001B[1m \u001B[0m│ \u001B[31m✗\u001B[0m          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_7  \u001B[0m\u001B[1m \u001B[0m│ \u001B[31m✗\u001B[0m          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_8  \u001B[0m\u001B[1m \u001B[0m│ \u001B[31m✗\u001B[0m          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_9  \u001B[0m\u001B[1m \u001B[0m│ \u001B[31m✗\u001B[0m          │     5.0s │\n",
       "├──────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1;3mAverages\u001B[0m\u001B[1m \u001B[0m│ 0.0% \u001B[32m✔\u001B[0m     │     5.0s │\n",
       "└──────────┴────────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "async def generate_bad_summary(news_inputs: str) -> str:\n",
    "    await asyncio.sleep(5) # Trying hard to avoid throttle limits\n",
    "    return 'The bond market rose by 33%'\n",
    "    \n",
    "report = dataset.evaluate_sync(generate_bad_summary, max_concurrency=1) # Limit concurrency to avoid throttle limits\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34080186-5f5d-41d2-9491-7119c9efa61e",
   "metadata": {},
   "source": [
    "As expected, performance is poor. Next, we'll implement something more sophosticated. An AI summariser that uses an LLM to create a structured output defined by the `Summary` Pydantic model, following the instructions in the system prompt to `Create a short, concise summary of the news.`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30b7def3-f42d-4844-8ef6-bbf66d45eb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06:03:02.414 evaluate generate_better_summary\n",
      "06:03:02.415   case: case_0\n",
      "06:03:02.415     execute generate_better_summary\n",
      "             evaluate generate_better_summary\n",
      "               case: case_0\n",
      "                 execute generate_better_summary\n",
      "06:03:07.419       summary_agent run\n",
      "06:03:07.421         chat us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "06:03:10.283     judge_input_output run\n",
      "06:03:10.283       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:03:13.322   case: case_1\n",
      "06:03:13.322     execute generate_better_summary\n",
      "             evaluate generate_better_summary\n",
      "               case: case_1\n",
      "                 execute generate_better_summary\n",
      "06:03:18.330       summary_agent run\n",
      "06:03:18.332         chat us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "             evaluate generate_better_summary\n",
      "               case: case_1\n",
      "                 execute generate_better_summary\n",
      "                   summary_agent run\n",
      "06:03:22.858         chat us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "             evaluate generate_better_summary\n",
      "               case: case_1\n",
      "06:03:25.474     judge_input_output run\n",
      "06:03:25.475       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:03:30.240   case: case_2\n",
      "06:03:30.240     execute generate_better_summary\n",
      "             evaluate generate_better_summary\n",
      "               case: case_2\n",
      "                 execute generate_better_summary\n",
      "06:03:35.244       summary_agent run\n",
      "06:03:35.246         chat us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "06:03:38.017     judge_input_output run\n",
      "06:03:38.018       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "             evaluate generate_better_summary\n",
      "06:03:49.171   case: case_3\n",
      "06:03:49.172     execute generate_better_summary\n",
      "06:03:54.176       summary_agent run\n",
      "06:03:54.179         chat us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "             evaluate generate_better_summary\n",
      "               case: case_3\n",
      "06:03:58.745     judge_input_output run\n",
      "06:03:58.746       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "             evaluate generate_better_summary\n",
      "06:04:03.264   case: case_4\n",
      "06:04:03.265     execute generate_better_summary\n",
      "06:04:08.268       summary_agent run\n",
      "06:04:08.272         chat us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "06:04:11.300     judge_input_output run\n",
      "06:04:11.300       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "             evaluate generate_better_summary\n",
      "06:04:14.781   case: case_5\n",
      "06:04:14.781     execute generate_better_summary\n",
      "             evaluate generate_better_summary\n",
      "               case: case_5\n",
      "                 execute generate_better_summary\n",
      "06:04:19.786       summary_agent run\n",
      "06:04:19.789         chat us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "             evaluate generate_better_summary\n",
      "               case: case_5\n",
      "06:04:23.022     judge_input_output run\n",
      "06:04:23.023       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "             evaluate generate_better_summary\n",
      "06:04:31.211   case: case_6\n",
      "06:04:31.212     execute generate_better_summary\n",
      "             evaluate generate_better_summary\n",
      "               case: case_6\n",
      "                 execute generate_better_summary\n",
      "06:04:36.215       summary_agent run\n",
      "06:04:36.220         chat us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "             evaluate generate_better_summary\n",
      "               case: case_6\n",
      "06:04:39.047     judge_input_output run\n",
      "06:04:39.049       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "             evaluate generate_better_summary\n",
      "06:04:48.410   case: case_7\n",
      "06:04:48.410     execute generate_better_summary\n",
      "             evaluate generate_better_summary\n",
      "               case: case_7\n",
      "                 execute generate_better_summary\n",
      "06:04:53.418       summary_agent run\n",
      "06:04:53.419         chat us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "06:04:57.380     judge_input_output run\n",
      "06:04:57.381       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "             evaluate generate_better_summary\n",
      "06:05:01.446   case: case_8\n",
      "06:05:01.446     execute generate_better_summary\n",
      "06:05:06.449       summary_agent run\n",
      "06:05:06.452         chat us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "             evaluate generate_better_summary\n",
      "               case: case_8\n",
      "06:05:10.870     judge_input_output run\n",
      "06:05:10.871       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "             evaluate generate_better_summary\n",
      "06:05:15.890   case: case_9\n",
      "06:05:15.890     execute generate_better_summary\n",
      "             evaluate generate_better_summary\n",
      "               case: case_9\n",
      "                 execute generate_better_summary\n",
      "06:05:20.892       summary_agent run\n",
      "06:05:20.894         chat us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "06:05:24.167     judge_input_output run\n",
      "06:05:24.168       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">        Evaluation Summary: generate_better_summary         </span>\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Case ID  </span>┃<span style=\"font-weight: bold\"> Metrics               </span>┃<span style=\"font-weight: bold\"> Assertions </span>┃<span style=\"font-weight: bold\"> Duration </span>┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"font-weight: bold\"> case_0   </span>│ input_tokens: 1408    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>          │     7.9s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 326    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_1   </span>│ input_tokens: 3608    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>          │    12.1s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 688    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 2           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_2   </span>│ input_tokens: 1418    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>          │     7.8s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 320    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_3   </span>│ input_tokens: 1428    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>          │     9.6s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 382    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_4   </span>│ input_tokens: 1416    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>          │     8.0s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 342    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_5   </span>│ input_tokens: 1404    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>          │     8.2s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 368    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_6   </span>│ input_tokens: 1400    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>          │     7.8s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 288    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_7   </span>│ input_tokens: 1418    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>          │     9.0s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 360    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_8   </span>│ input_tokens: 1410    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>          │     9.4s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 362    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_9   </span>│ input_tokens: 1396    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>          │     8.3s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 336    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> </span><span style=\"font-weight: bold; font-style: italic\">Averages</span><span style=\"font-weight: bold\"> </span>│ input_tokens: 1,630.6 │ 100.0% <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>   │     8.8s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 377.2  │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1.10        │            │          │\n",
       "└──────────┴───────────────────────┴────────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[3m        Evaluation Summary: generate_better_summary         \u001B[0m\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mCase ID \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mMetrics              \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mAssertions\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mDuration\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_0  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1408    │ \u001B[32m✔\u001B[0m          │     7.9s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 326    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_1  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 3608    │ \u001B[32m✔\u001B[0m          │    12.1s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 688    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 2           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_2  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1418    │ \u001B[32m✔\u001B[0m          │     7.8s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 320    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_3  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1428    │ \u001B[32m✔\u001B[0m          │     9.6s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 382    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_4  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1416    │ \u001B[32m✔\u001B[0m          │     8.0s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 342    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_5  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1404    │ \u001B[32m✔\u001B[0m          │     8.2s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 368    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_6  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1400    │ \u001B[32m✔\u001B[0m          │     7.8s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 288    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_7  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1418    │ \u001B[32m✔\u001B[0m          │     9.0s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 360    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_8  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1410    │ \u001B[32m✔\u001B[0m          │     9.4s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 362    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_9  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1396    │ \u001B[32m✔\u001B[0m          │     8.3s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 336    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1;3mAverages\u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1,630.6 │ 100.0% \u001B[32m✔\u001B[0m   │     8.8s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 377.2  │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1.10        │            │          │\n",
       "└──────────┴───────────────────────┴────────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    title: str\n",
    "    facts: list[str]\n",
    "    summary: str\n",
    "\n",
    "summary_agent = Agent(\n",
    "    'bedrock:us.anthropic.claude-3-5-haiku-20241022-v1:0', # Using a smaller, faster, cheaper but also less capable model\n",
    "    output_type=Summary,\n",
    "    system_prompt = (f\"\"\"Create a short, concise summary of the news article that prioritizes factual accuracy.\n",
    "Follow these guidelines:\n",
    "- Present only verifiable facts from the original text\n",
    "- Maintain the original meaning without distortion\n",
    "\n",
    "Respond with a structured output according to {Summary.model_json_schema()}\"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "async def generate_better_summary(news_inputs: str) -> Summary:\n",
    "    await asyncio.sleep(10) # Trying even harder to avoid throttle limits\n",
    "    r = await summary_agent.run({format_as_xml(news_inputs)})\n",
    "    return r.output.summary\n",
    "\n",
    "report = dataset.evaluate_sync(generate_better_summary, max_concurrency=1)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a75cabb-cc51-4d3a-ab82-139f200568f7",
   "metadata": {},
   "source": [
    "Now, we have better performance on the fact rubric, at least according to our evaluation dataset. Let's move onto evaluating and optimising another characteristic of our system. Imagine we are building a mobile app page where we can only show a maximum of 200 characters in the summary field. So, following our EDD principles, we can add another `Evaluator` which is a test for summary length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb89b76a-31f7-49d1-ab0b-c6afce05e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SummaryLengthEvaluator(Evaluator):\n",
    "    max_num_chars: int\n",
    "    async def evaluate(self, ctx: EvaluatorContext[str, str]) -> bool:  \n",
    "        if len(ctx.output) <= self.max_num_chars:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "dataset.add_evaluator(SummaryLengthEvaluator(200)) # Summary should be 200 characters or less"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db07ba2f-0bb3-4ee8-a694-ed6625a89108",
   "metadata": {},
   "source": [
    "Ideally, we add this evaluation **before** changing our prompt to provide instructions for this part of the task. This approach creates the baseline first, and makes it easier to identify any areas of performance trade-offs. Eventually, our prompt may be trying to achieve many things, some of which may interact. The evaluations will help spot regressions that could creep in by optimising for one area over another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eebe1d2-b98b-46cf-b6f8-b70832daed9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06:06:26.803 evaluate generate_better_summary\n",
      "06:06:26.804   case: case_0\n",
      "06:06:26.804     execute generate_better_summary\n",
      "06:06:31.808       summary_agent run\n",
      "06:06:31.810         chat us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "06:06:35.281     judge_input_output run\n",
      "06:06:35.282       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:06:38.689   case: case_1\n",
      "06:06:38.690     execute generate_better_summary\n",
      "06:06:43.694       summary_agent run\n",
      "06:06:43.696         chat us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "06:06:47.397     judge_input_output run\n",
      "06:06:47.398       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:06:50.896   case: case_2\n",
      "06:06:50.896     execute generate_better_summary\n",
      "06:06:55.901       summary_agent run\n",
      "06:06:55.902         chat us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "06:07:00.094     judge_input_output run\n",
      "06:07:00.095       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:07:03.850   case: case_3\n",
      "06:07:03.851     execute generate_better_summary\n",
      "06:07:08.855       summary_agent run\n",
      "06:07:08.857         chat us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "06:07:12.754     judge_input_output run\n",
      "06:07:12.754       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:07:16.911   case: case_4\n",
      "06:07:16.912     execute generate_better_summary\n",
      "06:07:21.916       summary_agent run\n",
      "06:07:21.918         chat us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "06:07:24.763     judge_input_output run\n",
      "06:07:24.764       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:07:27.598   case: case_5\n",
      "06:07:27.599     execute generate_better_summary\n",
      "06:07:32.603       summary_agent run\n",
      "06:07:32.605         chat us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "06:07:35.998     judge_input_output run\n",
      "06:07:35.999       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:07:40.144   case: case_6\n",
      "06:07:40.145     execute generate_better_summary\n",
      "06:07:45.149       summary_agent run\n",
      "06:07:45.152         chat us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "06:07:48.594     judge_input_output run\n",
      "06:07:48.595       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:07:53.322   case: case_7\n",
      "06:07:53.323     execute generate_better_summary\n",
      "06:07:58.326       summary_agent run\n",
      "06:07:58.330         chat us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "06:08:02.131     judge_input_output run\n",
      "06:08:02.132       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:08:06.177   case: case_8\n",
      "06:08:06.177     execute generate_better_summary\n",
      "06:08:11.179       summary_agent run\n",
      "06:08:11.180         chat us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "06:08:14.801     judge_input_output run\n",
      "06:08:14.802       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:08:18.932   case: case_9\n",
      "06:08:18.932     execute generate_better_summary\n",
      "06:08:23.934       summary_agent run\n",
      "06:08:23.935         chat us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "06:08:27.009     judge_input_output run\n",
      "06:08:27.010       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">        Evaluation Summary: generate_better_summary         </span>\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Case ID  </span>┃<span style=\"font-weight: bold\"> Metrics               </span>┃<span style=\"font-weight: bold\"> Assertions </span>┃<span style=\"font-weight: bold\"> Duration </span>┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"font-weight: bold\"> case_0   </span>│ input_tokens: 1408    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span><span style=\"color: #800000; text-decoration-color: #800000\">✗</span>         │     8.5s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 360    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_1   </span>│ input_tokens: 1384    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span><span style=\"color: #800000; text-decoration-color: #800000\">✗</span>         │     8.7s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 314    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_2   </span>│ input_tokens: 1418    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span><span style=\"color: #800000; text-decoration-color: #800000\">✗</span>         │     9.2s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 312    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_3   </span>│ input_tokens: 1428    │ <span style=\"color: #800000; text-decoration-color: #800000\">✗✗</span>         │     8.9s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 348    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_4   </span>│ input_tokens: 1416    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span><span style=\"color: #800000; text-decoration-color: #800000\">✗</span>         │     7.8s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 330    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_5   </span>│ input_tokens: 1404    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span><span style=\"color: #800000; text-decoration-color: #800000\">✗</span>         │     8.4s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 376    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_6   </span>│ input_tokens: 1400    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span><span style=\"color: #800000; text-decoration-color: #800000\">✗</span>         │     8.4s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 300    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_7   </span>│ input_tokens: 1418    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span><span style=\"color: #800000; text-decoration-color: #800000\">✗</span>         │     8.8s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 364    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_8   </span>│ input_tokens: 1410    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span><span style=\"color: #800000; text-decoration-color: #800000\">✗</span>         │     8.6s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 364    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_9   </span>│ input_tokens: 1396    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span><span style=\"color: #800000; text-decoration-color: #800000\">✗</span>         │     8.1s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 356    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> </span><span style=\"font-weight: bold; font-style: italic\">Averages</span><span style=\"font-weight: bold\"> </span>│ input_tokens: 1,408.2 │ 45.0% <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>    │     8.5s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 342.4  │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1.00        │            │          │\n",
       "└──────────┴───────────────────────┴────────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[3m        Evaluation Summary: generate_better_summary         \u001B[0m\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mCase ID \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mMetrics              \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mAssertions\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mDuration\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_0  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1408    │ \u001B[32m✔\u001B[0m\u001B[31m✗\u001B[0m         │     8.5s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 360    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_1  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1384    │ \u001B[32m✔\u001B[0m\u001B[31m✗\u001B[0m         │     8.7s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 314    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_2  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1418    │ \u001B[32m✔\u001B[0m\u001B[31m✗\u001B[0m         │     9.2s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 312    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_3  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1428    │ \u001B[31m✗\u001B[0m\u001B[31m✗\u001B[0m         │     8.9s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 348    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_4  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1416    │ \u001B[32m✔\u001B[0m\u001B[31m✗\u001B[0m         │     7.8s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 330    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_5  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1404    │ \u001B[32m✔\u001B[0m\u001B[31m✗\u001B[0m         │     8.4s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 376    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_6  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1400    │ \u001B[32m✔\u001B[0m\u001B[31m✗\u001B[0m         │     8.4s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 300    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_7  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1418    │ \u001B[32m✔\u001B[0m\u001B[31m✗\u001B[0m         │     8.8s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 364    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_8  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1410    │ \u001B[32m✔\u001B[0m\u001B[31m✗\u001B[0m         │     8.6s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 364    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_9  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1396    │ \u001B[32m✔\u001B[0m\u001B[31m✗\u001B[0m         │     8.1s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 356    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1;3mAverages\u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1,408.2 │ 45.0% \u001B[32m✔\u001B[0m    │     8.5s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 342.4  │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1.00        │            │          │\n",
       "└──────────┴───────────────────────┴────────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Simply re-running the same generation, but now with more evals\n",
    "report = dataset.evaluate_sync(generate_better_summary, max_concurrency=1)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89147187-99b9-43a3-86a2-f0203109c28e",
   "metadata": {},
   "source": [
    "Now, we can re-write our prompt to try and reduce the amount of jargon in our summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5efd39b8-f277-43a5-8bdb-9c22d4ede791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06:37:10.287 evaluate generate_concise_summary\n",
      "06:37:10.288   case: case_0\n",
      "06:37:10.288     execute generate_concise_summary\n",
      "06:37:25.291       concise_summary_agent run\n",
      "06:37:25.294         chat us.anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "06:37:29.216     judge_input_output run\n",
      "06:37:29.217       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:37:33.729   case: case_1\n",
      "06:37:33.730     execute generate_concise_summary\n",
      "06:37:48.732       concise_summary_agent run\n",
      "06:37:48.732         chat us.anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "06:37:52.695     judge_input_output run\n",
      "06:37:52.695       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:38:00.476   case: case_2\n",
      "06:38:00.477     execute generate_concise_summary\n",
      "06:38:15.479       concise_summary_agent run\n",
      "06:38:15.480         chat us.anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "06:38:18.371     judge_input_output run\n",
      "06:38:18.372       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:38:21.286   case: case_3\n",
      "06:38:21.286     execute generate_concise_summary\n",
      "06:38:36.288       concise_summary_agent run\n",
      "06:38:36.289         chat us.anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "06:38:40.643     judge_input_output run\n",
      "06:38:40.644       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:38:44.162   case: case_4\n",
      "06:38:44.162     execute generate_concise_summary\n",
      "06:38:59.163       concise_summary_agent run\n",
      "06:38:59.164         chat us.anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "06:39:02.914     judge_input_output run\n",
      "06:39:02.914       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:39:07.545   case: case_5\n",
      "06:39:07.546     execute generate_concise_summary\n",
      "06:39:22.548       concise_summary_agent run\n",
      "06:39:22.550         chat us.anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "06:39:31.919     judge_input_output run\n",
      "06:39:31.920       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:39:35.053   case: case_6\n",
      "06:39:35.054     execute generate_concise_summary\n",
      "06:39:50.058       concise_summary_agent run\n",
      "06:39:50.060         chat us.anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "06:40:02.816     judge_input_output run\n",
      "06:40:02.817       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:40:06.595   case: case_7\n",
      "06:40:06.595     execute generate_concise_summary\n",
      "06:40:21.599       concise_summary_agent run\n",
      "06:40:21.601         chat us.anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "06:40:34.464     judge_input_output run\n",
      "06:40:34.466       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:40:38.925   case: case_8\n",
      "06:40:38.926     execute generate_concise_summary\n",
      "06:40:53.928       concise_summary_agent run\n",
      "06:40:53.929         chat us.anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "06:40:59.859     judge_input_output run\n",
      "06:40:59.860       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "06:41:03.477   case: case_9\n",
      "06:41:03.478     execute generate_concise_summary\n",
      "06:41:18.482       concise_summary_agent run\n",
      "06:41:18.485         chat us.anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "06:41:30.596     judge_input_output run\n",
      "06:41:30.597       chat us.anthropic.claude-3-7-sonnet-20250219-v1:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">        Evaluation Summary: generate_concise_summary        </span>\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Case ID  </span>┃<span style=\"font-weight: bold\"> Metrics               </span>┃<span style=\"font-weight: bold\"> Assertions </span>┃<span style=\"font-weight: bold\"> Duration </span>┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"font-weight: bold\"> case_0   </span>│ input_tokens: 1410    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    18.9s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 260    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_1   </span>│ input_tokens: 1386    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    19.0s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 294    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_2   </span>│ input_tokens: 1420    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    17.9s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 270    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_3   </span>│ input_tokens: 1430    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    19.4s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 284    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_4   </span>│ input_tokens: 1418    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    18.7s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 264    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_5   </span>│ input_tokens: 1406    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    24.4s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 296    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_6   </span>│ input_tokens: 1402    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    27.8s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 228    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_7   </span>│ input_tokens: 1420    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    27.9s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 278    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_8   </span>│ input_tokens: 1412    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    20.9s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 282    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> case_9   </span>│ input_tokens: 1398    │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    27.1s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 286    │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> </span><span style=\"font-weight: bold; font-style: italic\">Averages</span><span style=\"font-weight: bold\"> </span>│ input_tokens: 1,410.2 │ 100.0% <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>   │    22.2s │\n",
       "│<span style=\"font-weight: bold\">          </span>│ output_tokens: 274.2  │            │          │\n",
       "│<span style=\"font-weight: bold\">          </span>│ requests: 1.00        │            │          │\n",
       "└──────────┴───────────────────────┴────────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[3m        Evaluation Summary: generate_concise_summary        \u001B[0m\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mCase ID \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mMetrics              \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mAssertions\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mDuration\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_0  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1410    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    18.9s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 260    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_1  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1386    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    19.0s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 294    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_2  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1420    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    17.9s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 270    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_3  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1430    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    19.4s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 284    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_4  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1418    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    18.7s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 264    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_5  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1406    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    24.4s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 296    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_6  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1402    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    27.8s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 228    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_7  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1420    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    27.9s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 278    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_8  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1412    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    20.9s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 282    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1mcase_9  \u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1398    │ \u001B[32m✔\u001B[0m\u001B[32m✔\u001B[0m         │    27.1s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 286    │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1           │            │          │\n",
       "├──────────┼───────────────────────┼────────────┼──────────┤\n",
       "│\u001B[1m \u001B[0m\u001B[1;3mAverages\u001B[0m\u001B[1m \u001B[0m│ input_tokens: 1,410.2 │ 100.0% \u001B[32m✔\u001B[0m   │    22.2s │\n",
       "│\u001B[1m          \u001B[0m│ output_tokens: 274.2  │            │          │\n",
       "│\u001B[1m          \u001B[0m│ requests: 1.00        │            │          │\n",
       "└──────────┴───────────────────────┴────────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "concise_summary_agent = Agent(\n",
    "    'bedrock:us.anthropic.claude-3-5-sonnet-20241022-v2:0', # Sonnet 3.5 is a very capable mid-sized model\n",
    "    output_type=Summary,\n",
    "    system_prompt = (f\"\"\"Create a short, concise summary of the news article that prioritizes factual accuracy.\n",
    "Follow these guidelines:\n",
    "- Present only verifiable facts from the original text\n",
    "- Maintain the original meaning without distortion\n",
    "\n",
    "<IMPORTANT>\n",
    "It's critical to generate one or two or three short sentences - but it must be less that 200 characters (about 30 words)!\n",
    "</IMPORTANT>\n",
    "\n",
    "Respond with a structured output according to {Summary.model_json_schema()}\"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "async def generate_concise_summary(news_inputs: str) -> Summary:\n",
    "    await asyncio.sleep(15) # Trying even harder to avoid throttle limits, longer sleep because of the shorter generation\n",
    "    r = await concise_summary_agent.run({format_as_xml(news_inputs)}, instrument=True)\n",
    "    return r.output.summary\n",
    "\n",
    "report = dataset.evaluate_sync(generate_concise_summary, max_concurrency=1)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb3159-734a-49cf-85a0-7ac7b92780b7",
   "metadata": {},
   "source": [
    "Now the tests pass! But, our application certainly isn't perfect. We could go on to ask if there's now missing important information that we're not capturing in the summary because it's too short, or maybe the tone of the summary isn't in line with bran guidelines. The process of engineering the evals goes on!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59adc58a-7a77-43ea-a227-6d3a3e59d0a0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Today you've seen a practical example of how to practice Eval Driven Development (EDD) for a finantial news summarisation use-case using the simple PydanticAI Evals framework. The key takeaway is how you can use a principled, structured methodology to building and evaluating with LLMs. I hope it's helped demistify this area and given you confidence to implement evals earlier in your AI application dev cycle. Happy building!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
