{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2bffd15a4e5f7af",
   "metadata": {},
   "source": [
    "---\n",
    "date: \"2025-2-14T14:00:00.00Z\"\n",
    "description: \"PydanticAI + Amazon Bedrock now supports images and documents\"\n",
    "published: true\n",
    "tags:\n",
    "  - python\n",
    "  - llm\n",
    "  - bedrock\n",
    "  - pydantic\n",
    "  - multimodal\n",
    "time_to_read: 5\n",
    "title: \"Multimodal PydanticAI + Amazon Bedrock\"\n",
    "type: post\n",
    "image: \"/public/images/pydantic-ai/multimodal.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a2e92b3dcd1fe",
   "metadata": {},
   "source": [
    "In addition to my [previous post](https://stephenhib.com/posts/pydantic-agents) introducing PydanticAI + Amazon Bedrock, the team have just released full support for images and documents in the latest `0.0.38` release."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4d87fb-ae2b-4988-89b5-978ccd2ef6f9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "535693b9dc169df2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:03:14.163420Z",
     "start_time": "2025-03-14T14:03:13.990694Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%uv pip install 'pydantic-ai-slim[bedrock]==0.0.39' 'pydantic-graph==0.0.39'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "951c33ec-9afc-4a4f-a4ad-8b552391ef56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:04:06.080925Z",
     "start_time": "2025-03-14T14:04:04.738107Z"
    }
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # This allows for nested event loops in Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d153a16734f1ca0",
   "metadata": {},
   "source": [
    "## ImageUrl\n",
    "First, let's see how we can pass an image URL directly in the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8afff7df467262cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:39:03.759826Z",
     "start_time": "2025-03-14T14:39:02.657319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The logo is from Python, a popular programming language known for its simplicity and versatility.\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent, ImageUrl\n",
    "\n",
    "agent = Agent(\n",
    "    model='bedrock:us.amazon.nova-pro-v1:0',\n",
    "    system_prompt='Be concise, reply with one sentence.'\n",
    ")\n",
    "\n",
    "result = agent.run_sync(\n",
    "    [\n",
    "        'What is this logo from?',\n",
    "        ImageUrl(url='https://www.python.org/static/img/python-logo.png'),\n",
    "    ]\n",
    ")\n",
    "print(result.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891256b61c4908e7",
   "metadata": {},
   "source": [
    "## DocumentUrl\n",
    "Alot of my work with customers is in back-office intelligent document processing use cases. Information is  encoded in one or more documents and should be decoded before it can be used by software. This is a deep and complex field full of edge cases and also highly domain specific. A pragmatic approach goes a long way, and starting simple is usually a good start. Anthropic provides [a nice starting point](https://docs.anthropic.com/en/docs/build-with-claude/pdf-support#how-pdf-support-works) for PDF support behind the API. This allows the caller to provide a PDF URL and Claude will decode the PDF for you into the format it needs to be tokenised into the LLM. PydanticAI wraps this with the [DocumentUrl](https://ai.pydantic.dev/api/messages/#pydantic_ai.messages.DocumentUrl) dataclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e6d6c12b1ed0b08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:40:25.996682Z",
     "start_time": "2025-03-14T14:40:20.607409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document provides a detailed overview of the new features and changes in Python 3.12, including language improvements, module updates, and API modifications.\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent, DocumentUrl\n",
    "\n",
    "agent = Agent(\n",
    "    model='bedrock:us.anthropic.claude-3-7-sonnet-20250219-v1:0',\n",
    "    system_prompt='Be concise, reply with one sentence.'\n",
    ")\n",
    "\n",
    "result = agent.run_sync(\n",
    "    [\n",
    "        'What is the main content of this document?',\n",
    "        DocumentUrl(url='https://docs.python.org/3.12/whatsnew/3.12.html'),\n",
    "    ]\n",
    ")\n",
    "print(result.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218523156b301e1d",
   "metadata": {},
   "source": [
    "However, there are limitations like the maximum request size, and the maximum pages per request. Furthermore, it's not possible to debug the intermediate format. For example imagine you have tables in your PDF and the LLM output is incorrect. Was the issue because the table was badly decoded from the PDF or was the issue that the LLM got confused by making an internal error? I'd suggest starting with the simplest option, convince yourself "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbc238c-cb91-4d3f-b569-8caef297d1e2",
   "metadata": {},
   "source": [
    "## BinaryContent\n",
    "If the image or document we're working with is only available locally we can also provide the binary content directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8d88d6e-8885-4d7f-be04-bf72774a3e7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:41:31.939118Z",
     "start_time": "2025-03-14T14:41:28.755342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'%PDF-1.7\\r\\n%\\xb5\\xb5\\xb5\\xb5\\r\\n1 0 obj\\r\\n<</Type/Catalog/Pages 2 0 R/Lang(en-US) /StructTreeRoot 153 0 R/MarkInfo<<'\n",
      "This is a presentation about good coding practices in Python, covering topics like PEP 8 style guidelines, documentation conventions, project organization, version control, and virtual environments.\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "\n",
    "from pydantic_ai import Agent, BinaryContent\n",
    "\n",
    "agent = Agent(\n",
    "    model='bedrock:us.anthropic.claude-3-5-haiku-20241022-v1:0',\n",
    "    system_prompt='Be concise, reply with one sentence.'\n",
    ")\n",
    "\n",
    "r = httpx.get('https://astropgh.github.io/astropgh-boot-camp-2020/seminars/coding_best_practices_2020-06-03.pdf')\n",
    "print(r.content[0:100])\n",
    "result = agent.run_sync(\n",
    "    [\n",
    "        'What is this?',\n",
    "        BinaryContent(data=r.content, media_type='application/pdf'),  \n",
    "    ]\n",
    ")\n",
    "print(result.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c89a9c-3c35-47c1-8ac4-a914045955ee",
   "metadata": {},
   "source": [
    "## Summary\n",
    "That's all for today, happy multi-modal building!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1acb9a17345480f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
